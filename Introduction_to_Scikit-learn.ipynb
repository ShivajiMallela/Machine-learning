{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6zSdJNepTcN"
   },
   "source": [
    "# Machine Learning with Python and Scikit-Learn\n",
    "\n",
    "This notebook documents my journey in learning machine learning using Python and Scikit-Learn. It includes various concepts, techniques, and practical implementations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7caBqv0qvcw8"
   },
   "source": [
    "## What is Scikit-Learn?\n",
    "\n",
    "Scikit-Learn, also known as `sklearn`, is one of the most popular open-source machine learning libraries for Python. It is built on top of NumPy, SciPy, and Matplotlib, and provides simple and efficient tools for data analysis and modeling.\n",
    "\n",
    "### Key Features of Scikit-Learn\n",
    "\n",
    "1. **Simple and Efficient Tools**: Scikit-Learn provides easy-to-use interfaces for various machine learning algorithms, making it simple to implement and experiment with models.\n",
    "\n",
    "2. **Wide Range of Algorithms**: The library includes a broad array of machine learning algorithms for classification, regression, clustering, and dimensionality reduction, as well as tools for model selection and evaluation.\n",
    "\n",
    "3. **Built on Powerful Libraries**: Scikit-Learn leverages the power of NumPy for numerical operations, SciPy for scientific computing, and Matplotlib for data visualization, ensuring efficient performance.\n",
    "\n",
    "4. **Community and Documentation**: Scikit-Learn has extensive documentation and a large community of users and contributors, making it easy to find resources, tutorials, and support.\n",
    "\n",
    "5. **Interoperability**: The library works well with other scientific and data manipulation libraries like Pandas, providing a seamless experience for data preprocessing and analysis.\n",
    "\n",
    "### Main Components of Scikit-Learn\n",
    "\n",
    "- **Datasets**: Tools for loading and generating sample datasets.\n",
    "- **Preprocessing**: Functions for scaling, transforming, and normalizing data.\n",
    "- **Model Selection**: Methods for splitting data into training and testing sets, as well as for cross-validation and hyperparameter tuning.\n",
    "- **Classification**: Algorithms such as Logistic Regression, k-Nearest Neighbors, Support Vector Machines, and more.\n",
    "- **Regression**: Algorithms like Linear Regression, Ridge, Lasso, and others.\n",
    "- **Clustering**: Methods including k-Means, DBSCAN, and Hierarchical Clustering.\n",
    "- **Dimensionality Reduction**: Techniques such as PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis).\n",
    "- **Model Evaluation**: Metrics and tools for evaluating model performance, including accuracy, precision, recall, and confusion matrices.\n",
    "\n",
    "### Example: Using Scikit-Learn\n",
    "\n",
    "Here is a simple example of how to use Scikit-Learn to load a dataset, preprocess data, train a model, and evaluate its performance.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "data['target'] = iris.target\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDBKt_nrpTcb"
   },
   "source": [
    "## What does this notebook cover?\n",
    "\n",
    "The Scikit-Learn library is very capable. However, learning everything off by heart isn't necessary. Instead, this notebook focuses some of the main use cases of the library.\n",
    "\n",
    "Specifically, I'll be exploring:\n",
    "\n",
    "<img src=\"images/sklearn-workflow-title.png\" alt=\"a 6 step scikit-learn workflow\"/>\n",
    "\n",
    "0. An end-to-end Scikit-Learn worfklow\n",
    "1. Getting the data ready\n",
    "2. Choosing the right maching learning estimator/aglorithm/model for your problem\n",
    "3. Fitting your chosen machine learning model to data and using it to make a prediction\n",
    "4. Evaluting a machine learning model\n",
    "5. Improving predictions through experimentation (hyperparameter tuning)\n",
    "6. Saving and loading a pretrained model\n",
    "7. Putting it all together in a pipeline\n",
    "\n",
    "> **Note:** All of the steps in this notebook are focused on [**supervised learning**](https://en.wikipedia.org/wiki/Supervised_learning) (having data and labels). The other side of supervised learning is [**unsupervised learning**](https://en.wikipedia.org/wiki/Unsupervised_learning) (having data but no labels)\n",
    "  \n",
    "By working through this notebook, I aim to build a solid foundation in Scikit-Learn that will allow me to progress further in my machine learning journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yABkEkPzpTcc"
   },
   "source": [
    "## Where Can I Get Help?\r\n",
    "\r\n",
    "If I get stuck or think of something I’d like to do that this notebook doesn’t cover, I have several options for seeking help:\r\n",
    "\r\n",
    "### Recommended Steps:\r\n",
    "\r\n",
    "1. **Try it** - Scikit-Learn is designed to be user-friendly. My first step should be to use what I know and try to figure out the answer to my question. Getting it wrong is part of the learning process. If in doubt, I’ll run my code and see what happens.\r\n",
    "\r\n",
    "2. **Press `SHIFT + TAB`** - I can view the docstring of a function (information on what the function does) by pressing `SHIFT + TAB` inside it. This is a good habit to develop. It will improve my research skills and give me a better understanding of the library.\r\n",
    "\r\n",
    "3. **Search for it** - If trying it on my own doesn’t work, I’ll search for my problem online. Since someone else has probably tried to do something similar, this step is often very helpful. I’ll likely end up in one of two places:\r\n",
    "   - **[Scikit-Learn documentation/user guide](http://scikit-learn.org/stable/documentation.html)** - This is the most extensive resource for Scikit-Learn information.\r\n",
    "   - **[Stack Overflow](https://stackoverflow.com/)** - This developer Q&A hub is full of questions and answers about different problems across a wide range of software development topics, and chances are, there’s one related to my problem.\r\n",
    "\r\n",
    "4. **ChatGPT** - ChatGPT can be very good at explaining code, though it can make mistakes. It’s best to verify the code it provides before using it. I might ask, “Can you explain the following code for me? {your code here}” and then follow up with more specific questions.\r\n",
    "\r\n",
    "#### Example Search:\r\n",
    "\r\n",
    "For example, if I need to tune the hyperparameters of a Scikit-Learn model, I could search for:\r\n",
    "\r\n",
    "\"how to tune the hyperparameters of a sklearn model\"\r\n",
    "\r\n",
    "Searching this on Google will lead me to the Scikit-Learn documentation for the `GridSearchCV` function: [Scikit-Learn Grid Search](http://scikit-learn.org/stable/modules/grid_search.html).\r\n",
    "\r\n",
    "The next steps here are to read through the documentation, check the examples, and see if they match the problem I’m trying to solve. If they do, I’ll rewrite the code to suit my needs, run it, and see what the outcomes are.\r\n",
    "\r\n",
    "5. **Ask for Help** - If I’ve gone through the above steps and I’m still stuck, I can ask my question on Stack Overflow or in relevant forums or communities, like the ZTM Machine Learning and AI Discord channel. I’ll be as specific as possible and provide details on what I’ve tried.\r\n",
    "\r\n",
    "### Remember:\r\n",
    "\r\n",
    "I don’t need to learn all of the functions by heart to begin with. What’s most important is continually asking myself, “What am I trying to do with the data?” I’ll start by answering that question and then practice finding the code that accomplishes it.\r\n",
    "\r\n",
    "### Getting Started:\r\n",
    "\r\n",
    "Let’s get started. First, I’ll import the libraries I’ve been using previously and check the version of Scikit-Learn.\r\n",
    "n of `sklearn` we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6JXxZQhpTcd",
    "outputId": "03549b97-89d0-422e-844a-9770be5fb42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Scikit-Learn version: 1.4.2 (materials in this notebook require this version or newer).\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "# %matplotlib inline # No longer required in newer versions of Jupyter (2022+)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "print(f\"Using Scikit-Learn version: {sklearn.__version__} (materials in this notebook require this version or newer).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BomdP0sIpTce"
   },
   "source": [
    "## 0. An End-to-End Scikit-Learn Workflow\r\n",
    "\r\n",
    "Before we get in-depth, let's quickly check out what an end-to-end Scikit-Learn workflow might look like.\r\n",
    "\r\n",
    "Once we've seen an end-to-end workflow, we'll dive into each step a little deeper.\r\n",
    "\r\n",
    "Specifically, I'll get hands-on with the following steps:\r\n",
    "1. Getting data ready (splitting into features and labels, preparing training and test sets)\r\n",
    "2. Choosing a model for the problem\r\n",
    "3. Fitting the model to the data and using it to make predictions\r\n",
    "4. Evaluating the model\r\n",
    "5. Experimenting to improve the model\r\n",
    "6. Saving the model for someone else to use\r\n",
    "\r\n",
    "> **Note:** The following section is information heavy but it is an end-to-end workflow. I'll go through it quite swiftly, but we'll break it down more throughout the rest of the notebook. Since Scikit-Learn is a vast library, capable of tackling many problems, the workflow I'm using is only one example of how it can be used.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Image Resources\n",
    "\n",
    "Throughout this notebook, I will be using various datasets and images to demonstrate machine learning concepts and techniques. All the datasets and images referenced in the examples can be found in the `data` and `images` folders respectively.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "- The `data` folder contains all the datasets used in this notebook.\n",
    "- Make sure to place the required datasets in the `data` folder before running the notebook.\n",
    "\n",
    "### Images\n",
    "\n",
    "- The `images` folder contains all the visual assets used for explanation and illustration purposes.\n",
    "- Ensure that the images are correctly placed in the `images` folder for proper visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9N_iOU7fpTcf"
   },
   "source": [
    "### Random Forest Classifier Workflow for Classifying Heart Disease\r\n",
    "\r\n",
    "#### 1. Get the Data Ready\r\n",
    "\r\n",
    "As an example dataset, we'll import `heart-disease.csv`.\r\n",
    "\r\n",
    "This file contains anonymized patient medical records and indicates whether they have heart disease or not. This is a classification problem since we're predicting whether a patient has heart disease (one thing) or not (another thing).\r\n",
    "ing).\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JQTQfegdpTcg",
    "outputId": "48f513b9-282f-483b-a9fa-3720194be40d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "heart_disease = pd.read_csv('data/heart-disease.csv')\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62-CddbPpTcg"
   },
   "source": [
    "Here, each row is a different patient and all columns except `target` are different patient characteristics.\n",
    "\n",
    "The `target` column indicates whether the patient has heart disease (`target=1`) or not (`target=0`), this is our \"label\" column, the variable we're going to try and predict.\n",
    "\n",
    "The rest of the columns (often called features) are what we'll be using to predict the `target` value.\n",
    "\n",
    "> **Note:** It's a common custom to save features to a varialbe `X` and labels to a variable `y`. In practice, we'd like to use the `X` (features) to build a predictive algorithm to predict the `y` (labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "39Kr2RO8pTch",
    "outputId": "f7c5c042-fd89-4a51-ad3e-2e41526cbae0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  \n",
       "0   0     1  \n",
       "1   0     2  \n",
       "2   0     2  \n",
       "3   0     2  \n",
       "4   0     2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create X (all the feature columns)\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "\n",
    "# Create y (the target column)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Check the head of the features DataFrame\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xjT-G0A8pTch",
    "outputId": "bfa63196-f770-48a4-9106-9fcc094d8d60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    1\n",
       " 1    1\n",
       " 2    1\n",
       " 3    1\n",
       " 4    1\n",
       " Name: target, dtype: int64,\n",
       " target\n",
       " 1    165\n",
       " 0    138\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the head and the value counts of the labels\n",
    "y.head(), y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw9V8XhupTci"
   },
   "source": [
    "One of the most important practices in machine learning is to split datasets into training and test sets.\n",
    "\n",
    "As in, a model will **train on the training set** to learn patterns and then those patterns can be **evaluated on the test set**.\n",
    "\n",
    "Crucially, a model should **never** see testing data during training.\n",
    "\n",
    "This is equivalent to a student studying course materials during the semester (training set) and then testing their abilities on the following exam (testing set).\n",
    "\n",
    "Scikit-learn provides the [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method to split datasets in training and test sets.\n",
    "\n",
    "> **Note:** A common practice to use an 80/20 or 70/30 or 75/25 split for training/testing data. There is also a third set, known as a validation set (e.g. 70/15/15 for training/validation/test) for hyperparamter tuning on but for now we'll focus on training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bVc8mBCppTci",
    "outputId": "93d9cac4-ef5b-4c2c-a43a-50c8db8c3cdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((227, 13), (76, 13), (227,), (76,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25) # by default train_test_split uses 25% of the data for the test set\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ClYzW1upTcj"
   },
   "source": [
    "#### 2. Choose the model and hyperparameters\n",
    "\n",
    "Choosing a model often depends on the type of problem you're working on.\n",
    "\n",
    "For example, there are different models that Scikit-Learn recommends whether you're working on a classification or regression problem.\n",
    "\n",
    "You can see a map breaking down the [different kinds of model options and recommendations in the Scikit-Learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "Scikit-Learn refers to models as \"estimators\", however, they are often also referred to as `model` or `clf` (short for classifier).\n",
    "\n",
    "A model's hyperparameters are settings you can change to adjust it for your problem, much like knobs on an oven you can tune to cook your favourite dish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RRapse_gpTcj"
   },
   "outputs": [],
   "source": [
    "# Since we're working on a classification problem, we'll start with a RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geaPXkg8pTck"
   },
   "source": [
    "We can see the current hyperparameters of a model with the [`get_params()`](https://scikit-learn.org/stable/developers/develop.html#get-params-and-set-params) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1iwjkZNPpTck",
    "outputId": "c12c5e80-fe75-42f2-d21b-cd76519611db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'monotonic_cst': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the current hyperparameters\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iydECOd0pTck"
   },
   "source": [
    "We'll leave this as is for now, as Scikit-Learn models generally have good default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCirMH6ppTcl"
   },
   "source": [
    "#### 3. Fit the model to the data and use it to make a prediction\n",
    "\n",
    "Fitting a model a dataset involves passing it the data and asking it to figure out the patterns.\n",
    "\n",
    "If there are labels (supervised learning), the model tries to work out the relationship between the data and the labels.\n",
    "\n",
    "If there are no labels (unsupervised learning), the model tries to find patterns and group similar samples together.\n",
    "\n",
    "Most Scikit-Learn models have the [`fit(X, y)`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit) method built-in, where the `X` parameter is the features and the `y` parameter is the labels.\n",
    "\n",
    "In our case, we start by fitting a model on the training split (`X_train`, `y_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7zN0XbAJpTcl",
    "outputId": "ad7f9606-4429-48f4-bbd2-6dc60a80b214"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npoP0_OWpTcm"
   },
   "source": [
    "#### Use the model to make a prediction\n",
    "\n",
    "The whole point of training a machine learning model is to use it to make some kind of prediction in the future.\n",
    "\n",
    "Once your model instance is trained, you can use the [`predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict) method to predict a target value given a set of features.\n",
    "\n",
    "In other words, use the model, along with some new, unseen and unlabelled data to predict the label.\n",
    "\n",
    "> **Note:** Data you predict on should be in the same shape and format as data you trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "q8AfTkdupTcm",
    "outputId": "47e12834-64a9-49d8-80ee-191a80aa1a6d"
   },
   "outputs": [],
   "source": [
    "# This doesn't work... incorrect shapes\n",
    "#y_label = clf.predict(np.array([0, 2, 3, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtJ2lrf3pTcm"
   },
   "source": [
    "Since our model was trained on data from `X_train`, predictions should be made on data in the same format and shape as `X_train`.\n",
    "\n",
    "Our goal in many machine learning problems is to use patterns learned from the training data to make predictions on the test data (or future unseen data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZGGn38M2pTcn",
    "outputId": "79fe60a3-703c-4e64-930f-c1122cfcfe22"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>288</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>115</td>\n",
       "      <td>564</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "223   56    0   0       200   288    1        0      133      1      4.0   \n",
       "24    40    1   3       140   199    0        1      178      1      1.4   \n",
       "277   57    1   1       124   261    0        1      141      0      0.3   \n",
       "155   58    0   0       130   197    0        1      131      0      0.6   \n",
       "85    67    0   2       115   564    0        0      160      0      1.6   \n",
       "\n",
       "     slope  ca  thal  \n",
       "223      0   2     3  \n",
       "24       2   0     3  \n",
       "277      2   0     3  \n",
       "155      1   0     2  \n",
       "85       1   0     3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to predict a label, data has to be in the same shape as X_train\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xDMQwfXipTcn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the model to make a prediction on the test data (further evaluation)\n",
    "y_preds = clf.predict(X=X_test)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZH6ttz-cpTco"
   },
   "source": [
    "#### 4. Evaluate the model\n",
    "\n",
    "Now we've made some predictions, we can start to use some more Scikit-Learn methods to figure out how good our model is.\n",
    "\n",
    "Each model or estimator has a built-in [`score()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.score) method.\n",
    "\n",
    "This method compares how well the model was able to learn the patterns between the features and labels.\n",
    "\n",
    "The `score()` method for each model uses a standard evaluation metric to measure your model's results.\n",
    "\n",
    "In the case of a classifier (our model), one of the most common evaluation metrics is [accuracy](https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score) (the fraction of correct predictions out of total predictions).\n",
    "\n",
    "Let's check out our model's accuracy on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "celfoNtOpTco",
    "outputId": "fc5b3058-734a-40ac-e1e1-75fcaaac2191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's accuracy on the training dataset is: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_acc = clf.score(X=X_train, y=y_train)\n",
    "print(f\"The model's accuracy on the training dataset is: {train_acc*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJzluJr0pTcp"
   },
   "source": [
    "Woah! Looks like our model does pretty well on the training datset.\n",
    "\n",
    "This is because it has a chance to see both data *and* labels.\n",
    "\n",
    "How about the test dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hNED7fCPpTcp",
    "outputId": "05114ef1-0e70-4168-9005-ba6d5714ba31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's accuracy on the testing dataset is: 80.26%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_acc = clf.score(X=X_test, y=y_test)\n",
    "print(f\"The model's accuracy on the testing dataset is: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMvwh4TZpTcp"
   },
   "source": [
    "Hmm, looks like our model's accuracy is a bit less on the test dataset than the training dataset.\n",
    "\n",
    "This is quite often the case, because remember, a model has never seen the testing examples before.\n",
    "\n",
    "There are also a number of other evaluation methods we can use for our classification models.\n",
    "\n",
    "All of the following classification metrics come from the [`sklearn.metrics`](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) module:\n",
    "* [`classification_report(y_true, y_true)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) - Builds a text report showing various classification metrics such as [precision, recall](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) and F1-score.\n",
    "* [`confusion_matrix(y_true, y_pred)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) - Create a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) to compare predictions to truth labels.\n",
    "* [`accuracy_score(y_true, y_pred)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) - Find the accuracy score (the default metric) for a classifier.\n",
    "\n",
    "All metrics have the following in common: they compare a model's predictions (`y_pred`) to truth labels (`y_true`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DdP7ASIApTcq",
    "outputId": "ac4b3e82-8198-4d0d-f49a-ea14492c394d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.73        28\n",
      "           1       0.84      0.85      0.85        48\n",
      "\n",
      "    accuracy                           0.80        76\n",
      "   macro avg       0.79      0.78      0.79        76\n",
      "weighted avg       0.80      0.80      0.80        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Create a classification report\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2dW12FBDpTcq",
    "outputId": "55a2350a-86a3-4e71-d1ae-50f4cf000d3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  8],\n",
       "       [ 7, 41]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_preds)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PoBIfkp0pTcr",
    "outputId": "58a85e69-1ca5-498c-e812-cec3ab4c39f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8026315789473685"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the accuracy score (same as the score() method for classifiers)\n",
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Building a Baseline Model and Experimentation\n",
    "\n",
    "When starting a machine learning project, the first model you build is often referred to as a **baseline model**. A baseline model is a simple model used to set a benchmark for future models. Sometimes, a baseline can be as simple as predicting the most common value in the dataset and then trying to improve upon it.\n",
    "\n",
    "##### Why Build a Baseline Model?\n",
    "\n",
    "- **Benchmarking**: Provides a reference point to compare more complex models.\n",
    "- **Simplicity**: Easy to understand and implement.\n",
    "- **Foundation**: Helps in understanding the problem and the dataset better.\n",
    "\n",
    "##### Improving the Baseline Model\n",
    "\n",
    "Once you've got a baseline model, like the one we've built here, it's important to remember that this is often not the final model you'll use. The next step in the workflow is to try and improve upon your baseline model. How? With one of the most important mottos in machine learning:\n",
    "\n",
    "**Experiment, experiment, experiment!**\n",
    "\n",
    "##### Types of Experiments\n",
    "\n",
    "Experiments can come in many different forms, but let's break it into two main categories:\n",
    "\n",
    "1. **Model Perspective**:\n",
    "    - Using a more complex model.\n",
    "    - Tuning your model's hyperparameters.\n",
    "    \n",
    "2. **Data Perspective**:\n",
    "    - Collecting more data.\n",
    "    - Improving the quality of existing data.\n",
    "\n",
    "If you're already working with an existing dataset, it’s often easier to try a series of model perspective experiments first and then turn to data perspective experiments if you aren't getting the results you're looking for.\n",
    "\n",
    "##### Cross-Validation\n",
    "\n",
    "When tuning a model’s hyperparameters, it’s crucial to ensure that your results are consistent across different training and test sets. This is where cross-validation comes in. Cross-validation is a technique that helps validate the results by using multiple versions of training and test sets rather than relying on a single split. This ensures that your results are not just due to chance.\n",
    "\n",
    "**Note**: Be cautious with cross-validation in time series problems, as you don’t want to mix samples from the future with samples from the past.\n",
    "\n",
    "##### Experimenting with Hyperparameters\n",
    "\n",
    "Different models have different hyperparameters you can tune. For our model, the `RandomForestClassifier()`, we'll start by trying different values for `n_estimators`, which is the number of trees in the random forest.\n",
    "\n",
    "By default, `n_estimators=100`. Let's try values from 100 to 200 and see what happens (generally, more trees can lead to better performance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pC_3eAJipTcs",
    "outputId": "903f0d3a-99b2-4bd2-fef4-272fc0d4807c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying model with 100 estimators...\n",
      "Model accuracy on test set: 81.58%\n",
      "\n",
      "Trying model with 110 estimators...\n",
      "Model accuracy on test set: 78.95%\n",
      "\n",
      "Trying model with 120 estimators...\n",
      "Model accuracy on test set: 80.26%\n",
      "\n",
      "Trying model with 130 estimators...\n",
      "Model accuracy on test set: 81.58%\n",
      "\n",
      "Trying model with 140 estimators...\n",
      "Model accuracy on test set: 80.26%\n",
      "\n",
      "Trying model with 150 estimators...\n",
      "Model accuracy on test set: 80.26%\n",
      "\n",
      "Trying model with 160 estimators...\n",
      "Model accuracy on test set: 81.58%\n",
      "\n",
      "Trying model with 170 estimators...\n",
      "Model accuracy on test set: 82.89%\n",
      "\n",
      "Trying model with 180 estimators...\n",
      "Model accuracy on test set: 78.95%\n",
      "\n",
      "Trying model with 190 estimators...\n",
      "Model accuracy on test set: 81.58%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different numbers of estimators (trees)... (no cross-validation)\n",
    "np.random.seed(42)\n",
    "for i in range(100, 200, 10):\n",
    "    print(f\"Trying model with {i} estimators...\")\n",
    "    model = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)\n",
    "    print(f\"Model accuracy on test set: {model.score(X_test, y_test) * 100:.2f}%\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__h1gMPepTcs"
   },
   "source": [
    "The metrics above were measured on a single train and test split.\n",
    "\n",
    "Let's use [`sklearn.model_selection.cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) to measure the results across 5 different train and test sets.\n",
    "\n",
    "We can achieve this by setting `cross_val_score(X, y, cv=5)`.\n",
    "\n",
    "Where `X` is the *full* feature set and `y` is the *full* label set and `cv` is the number of train and test splits `cross_val_score` will automatically create from the data (in our case, `5` different splits, this is known as 5-fold cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vESBlDvvpTcs",
    "outputId": "78df0eb8-fb97-4b94-8a7e-7b24aa24554c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying model with 100 estimators...\n",
      "Model accuracy on single test set split: 81.58%\n",
      "5-fold cross-validation score: 82.15%\n",
      "\n",
      "Trying model with 110 estimators...\n",
      "Model accuracy on single test set split: 77.63%\n",
      "5-fold cross-validation score: 81.17%\n",
      "\n",
      "Trying model with 120 estimators...\n",
      "Model accuracy on single test set split: 81.58%\n",
      "5-fold cross-validation score: 83.16%\n",
      "\n",
      "Trying model with 130 estimators...\n",
      "Model accuracy on single test set split: 80.26%\n",
      "5-fold cross-validation score: 83.14%\n",
      "\n",
      "Trying model with 140 estimators...\n",
      "Model accuracy on single test set split: 81.58%\n",
      "5-fold cross-validation score: 82.48%\n",
      "\n",
      "Trying model with 150 estimators...\n",
      "Model accuracy on single test set split: 81.58%\n",
      "5-fold cross-validation score: 80.17%\n",
      "\n",
      "Trying model with 160 estimators...\n",
      "Model accuracy on single test set split: 80.26%\n",
      "5-fold cross-validation score: 80.83%\n",
      "\n",
      "Trying model with 170 estimators...\n",
      "Model accuracy on single test set split: 81.58%\n",
      "5-fold cross-validation score: 81.83%\n",
      "\n",
      "Trying model with 180 estimators...\n",
      "Model accuracy on single test set split: 84.21%\n",
      "5-fold cross-validation score: 81.50%\n",
      "\n",
      "Trying model with 190 estimators...\n",
      "Model accuracy on single test set split: 81.58%\n",
      "5-fold cross-validation score: 81.83%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# With cross-validation\n",
    "np.random.seed(42)\n",
    "for i in range(100, 200, 10):\n",
    "    print(f\"Trying model with {i} estimators...\")\n",
    "    model = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)\n",
    "\n",
    "    # Measure the model score on a single train/test split\n",
    "    model_score = model.score(X_test, y_test)\n",
    "    print(f\"Model accuracy on single test set split: {model_score * 100:.2f}%\")\n",
    "\n",
    "    # Measure the mean cross-validation score across 5 different train and test splits\n",
    "    cross_val_mean = np.mean(cross_val_score(model, X, y, cv=5))\n",
    "    print(f\"5-fold cross-validation score: {cross_val_mean * 100:.2f}%\")\n",
    "\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6y7tXMWpTc3"
   },
   "source": [
    "Which model had the best cross-validation score?\n",
    "\n",
    "This is usually a better indicator of a quality model than a single split accuracy score.\n",
    "\n",
    "Rather than set up and track the results of these experiments manually, we can get Scikit-Learn to do the exploration for us.\n",
    "\n",
    "Scikit-Learn's [`sklearn.model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) is a way to search over a set of different hyperparameter values and automatically track which perform the best.\n",
    "\n",
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Cf1pGtPupTc3",
    "outputId": "c376a569-a251-405a-f020-7617bf7657d9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "The best parameter values are: {'n_estimators': 120}\n",
      "With a score of: 82.82%\n"
     ]
    }
   ],
   "source": [
    "# Another way to do it with GridSearchCV...\n",
    "np.random.seed(42)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameters to search over in dictionary form\n",
    "# (these can be any of your target model's hyperparameters)\n",
    "param_grid = {'n_estimators': [i for i in range(100, 200, 10)]}\n",
    "\n",
    "# Setup the grid search\n",
    "grid = GridSearchCV(estimator=RandomForestClassifier(),\n",
    "                    param_grid=param_grid,\n",
    "                    cv=5,\n",
    "                    verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Find the best parameters\n",
    "print(f\"The best parameter values are: {grid.best_params_}\")\n",
    "print(f\"With a score of: {grid.best_score_*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5OhuV5BpTc4"
   },
   "source": [
    "We can extract the best model/estimator with the `best_estimator_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7a2erfdBpTc4",
    "outputId": "3be3617e-c330-4bba-f501-913dd6a135af"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=120)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_estimators=120)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=120)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to be the best estimator\n",
    "clf = grid.best_estimator_\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLiYomMepTc4"
   },
   "source": [
    "And now we've got the best cross-validated model, we can fit and score it on our original single train/test split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XK13L1nkpTc5",
    "outputId": "bf6d1e0c-03cd-4713-926b-2937dfe09fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model score on single split of the data: 80.26%\n"
     ]
    }
   ],
   "source": [
    "# Fit the best model\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Find the best model scores on our single test split\n",
    "# (note: this may be lower than the cross-validation score since it's only on one split of the data)\n",
    "print(f\"Best model score on single split of the data: {clf.score(X_test, y_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb0RE7CKpTc5"
   },
   "source": [
    "#### 6. Save a model for someone else to use\n",
    "\n",
    "When you've done a few experiments and you're happy with how your model is doing, you'll likely want someone else to be able to use it.\n",
    "\n",
    "This may come in the form of a teammate or colleague trying to replicate and validate your results or through a customer using your model as part of a service or application you offer.\n",
    "\n",
    "Saving a model also allows you to reuse it later without having to go through retraining it. Which is helpful, especially when your training times start to increase.\n",
    "\n",
    "You can [save a Scikit-Learn model](https://scikit-learn.org/stable/model_persistence.html) using Python's in-built [`pickle` module](https://docs.python.org/3/library/pickle.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "QJaAxsj-pTc5"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save an existing model to file\n",
    "pickle.dump(model, open(\"random_forest_model_1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4D7-0PfmpTc6",
    "outputId": "5f2c043d-81d2-4bf4-987a-bb8ed2324cd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pickle model prediction score: 81.58%\n"
     ]
    }
   ],
   "source": [
    "# Load a saved pickle model and evaluate it\n",
    "loaded_pickle_model = pickle.load(open(\"random_forest_model_1.pkl\", \"rb\"))\n",
    "print(f\"Loaded pickle model prediction score: {loaded_pickle_model.score(X_test, y_test) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzhT9UwkpTc6"
   },
   "source": [
    "For larger models, it may be more efficient to use [Joblib](https://joblib.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1FwgP2G_pTc6",
    "outputId": "368b97dd-06fa-4f75-aded-16130def7f46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model_1.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Save a model using joblib\n",
    "dump(model, \"random_forest_model_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wmjCQitApTc7",
    "outputId": "02867ed5-0649-4075-c240-5685e9297570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded joblib model prediction score: 81.58%\n"
     ]
    }
   ],
   "source": [
    "# Load a saved joblib model and evaluate it\n",
    "loaded_joblib_model = load(\"random_forest_model_1.joblib\")\n",
    "print(f\"Loaded joblib model prediction score: {loaded_joblib_model.score(X_test, y_test) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGnUWZqlpTc7"
   },
   "source": [
    "Woah!\n",
    "\n",
    "We've covered a lot of ground fast...\n",
    "\n",
    "Let's break things down a bit more by revisting each section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5fYm-rwpTc7"
   },
   "source": [
    "## 1. Getting the data ready\n",
    "\n",
    "Data doesn't always come ready to use with a Scikit-Learn machine learning model.\n",
    "\n",
    "Three of the main steps you'll often have to take are:\n",
    "* Splitting the data into features (usually `X`) and labels (usually `y`).\n",
    "* Splitting the data into training and testing sets (and possibly a validation set).\n",
    "* Filling (also called imputing) or disregarding missing values.\n",
    "* Converting non-numerical values to numerical values (also call feature encoding).\n",
    "\n",
    "Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dT1e-B6CpTc8",
    "outputId": "554f158e-971b-4507-e3f7-f62f71dbdce5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data into X & y\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xc802sVMpTc8",
    "outputId": "c9628c6d-9d22-4ec2-f62c-5227f2116a04"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     63    1   3       145   233    1        0      150      0      2.3   \n",
       "1     37    1   2       130   250    0        1      187      0      3.5   \n",
       "2     41    0   1       130   204    0        0      172      0      1.4   \n",
       "3     56    1   1       120   236    0        1      178      0      0.8   \n",
       "4     57    0   0       120   354    0        1      163      1      0.6   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  \n",
       "0        0   0     1  \n",
       "1        0   0     2  \n",
       "2        2   0     2  \n",
       "3        2   0     2  \n",
       "4        2   0     2  \n",
       "..     ...  ..   ...  \n",
       "298      1   0     3  \n",
       "299      1   0     3  \n",
       "300      1   2     3  \n",
       "301      1   1     3  \n",
       "302      1   1     2  \n",
       "\n",
       "[303 rows x 13 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data into features (X) and labels (y)\n",
    "X = heart_disease.drop('target', axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht5HeLcwpTc8"
   },
   "source": [
    "Nice! Looks like our dataset has 303 samples with 13 features (13 columns).\n",
    "\n",
    "Let's check out the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1l_lZJuFpTc9",
    "outputId": "29aa4caf-45d2-4e45-ce94-e8f86376eaa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "298    0\n",
       "299    0\n",
       "300    0\n",
       "301    0\n",
       "302    0\n",
       "Name: target, Length: 303, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = heart_disease['target']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axef418BpTc9"
   },
   "source": [
    "303 labels with values of `0` (no heart disease) and `1` (heart disease).\n",
    "\n",
    "Now let's split our data into training and test sets, we'll use an 80/20 split (80% of samples for training and 20% of samples for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "uTqO-CoopTc9",
    "outputId": "4c78d1df-5aeb-4cdc-9df3-3bacf43c93b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((242, 13), (61, 13), (242,), (61,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2) # you can change the test size\n",
    "\n",
    "# Check the shapes of different data splits\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "153M54h5pTc-",
    "outputId": "d5b846ee-f073-496a-cfd0-145b69f8cd4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242.4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80% of data is being used for the training set (the model will learn patterns on these samples)\n",
    "X.shape[0] * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "4f13MLlApTc-",
    "outputId": "57176c11-5343-4b98-82f8-f6e2c429f143"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And 20% of the data is being used for the testing set (the model will be evaluated on these samples)\n",
    "X.shape[0] * 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27QqEMNGpTc_"
   },
   "source": [
    "### 1.1 Make sure it's all numerical\n",
    "\n",
    "Computers love numbers.\n",
    "\n",
    "So one thing you'll often have to make sure of is that your datasets are in numerical form.\n",
    "\n",
    "This even goes for datasets which contain non-numerical features that you may want to include in a model.\n",
    "\n",
    "For example, if we were working with a car sales dataset, how might we turn features such as `Make` and `Colour` into numbers?\n",
    "\n",
    "Let's figure it out.\n",
    "\n",
    "First, we'll import the `car-sales-extended.csv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "P-5nRR9ZpTc_",
    "outputId": "acb0b0ed-d111-4a0d-8154-9917691ecd14"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Odometer (KM)</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>35431</td>\n",
       "      <td>4</td>\n",
       "      <td>15323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Blue</td>\n",
       "      <td>192714</td>\n",
       "      <td>5</td>\n",
       "      <td>19943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>84714</td>\n",
       "      <td>4</td>\n",
       "      <td>28343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>154365</td>\n",
       "      <td>4</td>\n",
       "      <td>13434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>181577</td>\n",
       "      <td>3</td>\n",
       "      <td>14043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Black</td>\n",
       "      <td>35820</td>\n",
       "      <td>4</td>\n",
       "      <td>32042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>White</td>\n",
       "      <td>155144</td>\n",
       "      <td>3</td>\n",
       "      <td>5716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>66604</td>\n",
       "      <td>4</td>\n",
       "      <td>31570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>215883</td>\n",
       "      <td>4</td>\n",
       "      <td>4001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Blue</td>\n",
       "      <td>248360</td>\n",
       "      <td>4</td>\n",
       "      <td>12732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Make Colour  Odometer (KM)  Doors  Price\n",
       "0     Honda  White          35431      4  15323\n",
       "1       BMW   Blue         192714      5  19943\n",
       "2     Honda  White          84714      4  28343\n",
       "3    Toyota  White         154365      4  13434\n",
       "4    Nissan   Blue         181577      3  14043\n",
       "..      ...    ...            ...    ...    ...\n",
       "995  Toyota  Black          35820      4  32042\n",
       "996  Nissan  White         155144      3   5716\n",
       "997  Nissan   Blue          66604      4  31570\n",
       "998   Honda  White         215883      4   4001\n",
       "999  Toyota   Blue         248360      4  12732\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import car-sales-extended.csv\n",
    "car_sales = pd.read_csv(\"data/car-sales-extended.csv\")\n",
    "car_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOV3cTbGpTc_"
   },
   "source": [
    "We can check the dataset types with `.dtypes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "whibTHAapTdA",
    "outputId": "6bdaba7b-c57c-41a3-92d6-e0394693c96e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             object\n",
       "Colour           object\n",
       "Odometer (KM)     int64\n",
       "Doors             int64\n",
       "Price             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_sales.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OtFK9EfpTdA"
   },
   "source": [
    "Notice the `Make` and `Colour` features are of `dtype=object` (they're strings) where as the rest of the columns are of `dtype=int64`.\n",
    "\n",
    "If we want to use the `Make` and `Colour` features in our model, we'll need to figure out how to turn them into numerical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "RBIgyOSRpTdB"
   },
   "outputs": [],
   "source": [
    "# Split into X & y and train/test\n",
    "X = car_sales.drop(\"Price\", axis=1)\n",
    "y = car_sales[\"Price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QrtTuaJpTdB"
   },
   "source": [
    "Now let's try and build a model on our `car_sales` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "sg5K9IJlpTdB",
    "outputId": "c0888b0a-601b-43f7-e4df-6553efe6696c"
   },
   "outputs": [],
   "source": [
    "# # Try to predict with random forest on price column (doesn't work)\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# model = RandomForestRegressor()\n",
    "# model.fit(X_train, y_train)\n",
    "# model.score(X_test, y_test)\n",
    "\n",
    "#comment out this and run, to try and see what the output is.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgUqvGxdpTdC"
   },
   "source": [
    "Oops... this doesn't work, we'll have to convert the non-numerical features into numbers first.\n",
    "\n",
    "The process of turning categorical features into numbers is often referred to as **encoding**.\n",
    "\n",
    "Scikit-Learn has a fantastic in-depth guide on [*Encoding categorical features*](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features).\n",
    "\n",
    "But let's look at one of the most straightforward ways to turn categorical features into numbers, [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n",
    "\n",
    "In machine learning, [one-hot encoding](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics) gives a value of `1` to the target value and a value of `0` to the other values.\n",
    "\n",
    "For example, let's say we had five samples and three car make options, Honda, Toyota, BMW.\n",
    "\n",
    "And our samples were:\n",
    "1. Honda\n",
    "2. BMW\n",
    "3. BMW\n",
    "4. Toyota\n",
    "5. Toyota\n",
    "\n",
    "If we were to one-hot encode these, it would look like:\n",
    "\n",
    "| Sample | Honda | Toyota | BMW |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1 | 1 | 0 | 0 |\n",
    "| 2 | 0 | 0 | 1 |\n",
    "| 3 | 0 | 0 | 1 |\n",
    "| 4 | 0 | 1 | 0 |\n",
    "| 5 | 0 | 1 | 0 |\n",
    "\n",
    "Notice how there's a 1 for each target value but a 0 for each other value.\n",
    "\n",
    "We can use the following steps to one-hot encode our dataset:\n",
    "1. Import [`sklearn.preprocessing.OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to one-hot encode our features and [`sklearn.compose.ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) to target the specific columns of our DataFrame to transform.\n",
    "2. Define the categorical features we'd like to transform.\n",
    "3. Create an instance of the `OneHotEncoder`.\n",
    "4. Create an instance of `ColumnTransformer` and feed it the transforms we'd like to make.\n",
    "5. Fit the instance of the `ColumnTransformer` to our data and transform it with the [`fit_transform(X)`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer.fit_transform) method.\n",
    "\n",
    "> **Note:** In Scikit-Learn, the term \"transformer\" is often used to refer to something that *transforms* data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4DjE0vk_pTdC",
    "outputId": "c6662d3f-5bf4-4157-b597-d55da8e9a231"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 3.54310e+04],\n",
       "       [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n",
       "        1.00000e+00, 1.92714e+05],\n",
       "       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 8.47140e+04],\n",
       "       ...,\n",
       "       [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 6.66040e+04],\n",
       "       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 2.15883e+05],\n",
       "       [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 2.48360e+05]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Import OneHotEncoder and ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# 2. Define the categorical features to transform\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "\n",
    "# 3. Create an instance of OneHotEncoder\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "# 4. Create an instance of ColumnTransformer\n",
    "transformer = ColumnTransformer([(\"one_hot\", # name\n",
    "                                  one_hot, # transformer\n",
    "                                  categorical_features)], # columns to transform - You can also pass '[\"Make\", \"Colour\", \"Doors\"]' directly inplace of 'categorical_features'. \n",
    "                                  remainder=\"passthrough\") # what to do with the rest of the columns? (\"passthrough\" = leave unchanged)\n",
    "\n",
    "# 5. Turn the categorical features into numbers (this will return an array-like sparse matrix, not a DataFrame)\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0r3ihrukpTdD"
   },
   "source": [
    "> **Note:** You might be thinking why we considered `Doors` as a categorical variable. Which is a good question considering `Doors` is already numerical. Well, the answer is that `Doors` could be either numerical or categorical. For example, you can shop for 4-door cars or shop for 5-door cars (which always confused me since where's the 5th door?). However, you could experiment with treating this value as numerical or categorical, training a model on each, and then see how each model performs.\n",
    "\n",
    "Woah! Looks like our samples are all numerical, what did our data look like previously?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "RmV9GMEipTdD",
    "outputId": "fa968e3a-072b-4a05-cf2d-e286cd829527"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Odometer (KM)</th>\n",
       "      <th>Doors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>35431</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Blue</td>\n",
       "      <td>192714</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>84714</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>154365</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>181577</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Make Colour  Odometer (KM)  Doors\n",
       "0   Honda  White          35431      4\n",
       "1     BMW   Blue         192714      5\n",
       "2   Honda  White          84714      4\n",
       "3  Toyota  White         154365      4\n",
       "4  Nissan   Blue         181577      3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMZNooAdpTdI"
   },
   "source": [
    "It seems `OneHotEncoder` and `ColumnTransformer` have turned all of our data samples into numbers.\n",
    "\n",
    "Let's check out the first transformed sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "tBsKWfs6pTdI",
    "outputId": "df895dd9-d262-4834-b438-b56c1675f436"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "       1.0000e+00, 0.0000e+00, 3.5431e+04])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first transformed sample\n",
    "transformed_X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cL2FsuvpTdJ"
   },
   "source": [
    "And what were these values originally?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "zekbKRHtpTdJ",
    "outputId": "cb1eddd7-6bd8-40a1-b791-5aed1c76521a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             Honda\n",
       "Colour           White\n",
       "Odometer (KM)    35431\n",
       "Doors                4\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View original first sample\n",
    "X.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqUpqWZqpTdK"
   },
   "source": [
    "#### 1.1.1 Numerically encoding data with pandas\n",
    "\n",
    "Another way we can numerically encode data is directly with pandas.\n",
    "\n",
    "We can use the [`pandas.get_dummies()`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) (or `pd.get_dummies()` for short) method and then pass it our target columns.\n",
    "\n",
    "In return, we'll get a one-hot encoded version of our target columns.\n",
    "\n",
    "Let's remind ourselves of what our DataFrame looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "yMNSgUeBpTdK",
    "outputId": "71956d16-4a0e-4db4-9228-978b847b8fe7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Odometer (KM)</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>35431</td>\n",
       "      <td>4</td>\n",
       "      <td>15323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Blue</td>\n",
       "      <td>192714</td>\n",
       "      <td>5</td>\n",
       "      <td>19943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>84714</td>\n",
       "      <td>4</td>\n",
       "      <td>28343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>154365</td>\n",
       "      <td>4</td>\n",
       "      <td>13434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>181577</td>\n",
       "      <td>3</td>\n",
       "      <td>14043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Make Colour  Odometer (KM)  Doors  Price\n",
       "0   Honda  White          35431      4  15323\n",
       "1     BMW   Blue         192714      5  19943\n",
       "2   Honda  White          84714      4  28343\n",
       "3  Toyota  White         154365      4  13434\n",
       "4  Nissan   Blue         181577      3  14043"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View head of original DataFrame\n",
    "car_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X64DNmsFpTdL"
   },
   "source": [
    "Wonderful, now let's use `pd.get_dummies()` to turn our categorical variables into one-hot encoded variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ZUfsahyjpTdL",
    "outputId": "e739e323-2d31-4bb2-92f7-5a0982d1151a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doors</th>\n",
       "      <th>Make_BMW</th>\n",
       "      <th>Make_Honda</th>\n",
       "      <th>Make_Nissan</th>\n",
       "      <th>Make_Toyota</th>\n",
       "      <th>Colour_Black</th>\n",
       "      <th>Colour_Blue</th>\n",
       "      <th>Colour_Green</th>\n",
       "      <th>Colour_Red</th>\n",
       "      <th>Colour_White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Doors  Make_BMW  Make_Honda  Make_Nissan  Make_Toyota  Colour_Black  \\\n",
       "0        4     False        True        False        False         False   \n",
       "1        5      True       False        False        False         False   \n",
       "2        4     False        True        False        False         False   \n",
       "3        4     False       False        False         True         False   \n",
       "4        3     False       False         True        False         False   \n",
       "..     ...       ...         ...          ...          ...           ...   \n",
       "995      4     False       False        False         True          True   \n",
       "996      3     False       False         True        False         False   \n",
       "997      4     False       False         True        False         False   \n",
       "998      4     False        True        False        False         False   \n",
       "999      4     False       False        False         True         False   \n",
       "\n",
       "     Colour_Blue  Colour_Green  Colour_Red  Colour_White  \n",
       "0          False         False       False          True  \n",
       "1           True         False       False         False  \n",
       "2          False         False       False          True  \n",
       "3          False         False       False          True  \n",
       "4           True         False       False         False  \n",
       "..           ...           ...         ...           ...  \n",
       "995        False         False       False         False  \n",
       "996        False         False       False          True  \n",
       "997         True         False       False         False  \n",
       "998        False         False       False          True  \n",
       "999         True         False       False         False  \n",
       "\n",
       "[1000 rows x 10 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode categorical variables\n",
    "categorical_variables = [\"Make\", \"Colour\", \"Doors\"]\n",
    "dummies = pd.get_dummies(data=car_sales[categorical_variables])\n",
    "dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UwMX4X-pTdM"
   },
   "source": [
    "Nice!\n",
    "\n",
    "Notice how there's a new column for each categorical option (e.g. `Make_BMW`, `Make_Honda`, etc).\n",
    "\n",
    "But also notice how it also missed the `Doors` column?\n",
    "\n",
    "This is because `Doors` is already numeric, so for `pd.get_dummies()` to work on it, we can change it to type `object`.\n",
    "\n",
    "By default, `pd.get_dummies()` also turns all of the values to bools (`True` or `False`).\n",
    "\n",
    "We can get the returned values as `0` or `1` by setting `dtype=float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "xr05nqIlpTdM",
    "outputId": "b17ba1d3-2b01-44a7-af44-40c73fb80391"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make_BMW</th>\n",
       "      <th>Make_Honda</th>\n",
       "      <th>Make_Nissan</th>\n",
       "      <th>Make_Toyota</th>\n",
       "      <th>Colour_Black</th>\n",
       "      <th>Colour_Blue</th>\n",
       "      <th>Colour_Green</th>\n",
       "      <th>Colour_Red</th>\n",
       "      <th>Colour_White</th>\n",
       "      <th>Doors_3</th>\n",
       "      <th>Doors_4</th>\n",
       "      <th>Doors_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Make_BMW  Make_Honda  Make_Nissan  Make_Toyota  Colour_Black  \\\n",
       "0         0.0         1.0          0.0          0.0           0.0   \n",
       "1         1.0         0.0          0.0          0.0           0.0   \n",
       "2         0.0         1.0          0.0          0.0           0.0   \n",
       "3         0.0         0.0          0.0          1.0           0.0   \n",
       "4         0.0         0.0          1.0          0.0           0.0   \n",
       "..        ...         ...          ...          ...           ...   \n",
       "995       0.0         0.0          0.0          1.0           1.0   \n",
       "996       0.0         0.0          1.0          0.0           0.0   \n",
       "997       0.0         0.0          1.0          0.0           0.0   \n",
       "998       0.0         1.0          0.0          0.0           0.0   \n",
       "999       0.0         0.0          0.0          1.0           0.0   \n",
       "\n",
       "     Colour_Blue  Colour_Green  Colour_Red  Colour_White  Doors_3  Doors_4  \\\n",
       "0            0.0           0.0         0.0           1.0      0.0      1.0   \n",
       "1            1.0           0.0         0.0           0.0      0.0      0.0   \n",
       "2            0.0           0.0         0.0           1.0      0.0      1.0   \n",
       "3            0.0           0.0         0.0           1.0      0.0      1.0   \n",
       "4            1.0           0.0         0.0           0.0      1.0      0.0   \n",
       "..           ...           ...         ...           ...      ...      ...   \n",
       "995          0.0           0.0         0.0           0.0      0.0      1.0   \n",
       "996          0.0           0.0         0.0           1.0      1.0      0.0   \n",
       "997          1.0           0.0         0.0           0.0      0.0      1.0   \n",
       "998          0.0           0.0         0.0           1.0      0.0      1.0   \n",
       "999          1.0           0.0         0.0           0.0      0.0      1.0   \n",
       "\n",
       "     Doors_5  \n",
       "0        0.0  \n",
       "1        1.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "..       ...  \n",
       "995      0.0  \n",
       "996      0.0  \n",
       "997      0.0  \n",
       "998      0.0  \n",
       "999      0.0  \n",
       "\n",
       "[1000 rows x 12 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have to convert doors to object for dummies to work on it...\n",
    "car_sales[\"Doors\"] = car_sales[\"Doors\"].astype(object)\n",
    "dummies = pd.get_dummies(data=car_sales[[\"Make\", \"Colour\", \"Doors\"]],\n",
    "                         dtype=float)\n",
    "dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1PCECtfpTdN"
   },
   "source": [
    "Woohoo!\n",
    "\n",
    "We've now turned our data into fully numeric form using Scikit-Learn and pandas.\n",
    "\n",
    "Now you might be wondering...\n",
    "\n",
    "**Should you use Scikit-Learn or pandas for turning data into numerical form?**\n",
    "\n",
    "And the answer is either.\n",
    "\n",
    "But as a rule of thumb:\n",
    "* If you're performing **quick data analysis and running small modelling experiments**, use `pandas` as it's generally quite fast to get up and running.\n",
    "* If you're performing a **larger scale modelling experiment** or would like to put your **data processing steps into a production pipeline**, I'd recommend leaning towards Scikit-Learn, specifically a [Scikit-Learn Pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline) (chaining together multiple estimator/modelling steps).\n",
    "\n",
    "Since we've turned our data into numerical form, how about we try and fit our model again?\n",
    "\n",
    "Let's recreate a train/test split except this time we'll use `transformed_X` instead of `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "SsH0xjc0pTdN",
    "outputId": "130748cc-5e78-4ec0-c07c-9909e068ab9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3235867221569877"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Create train and test splits with transformed_X\n",
    "X_train, X_test, y_train, y_test = train_test_split(transformed_X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Create the model instance\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Fit the model on the numerical data (this errored before since our data wasn't fully numeric)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Score the model (returns r^2 metric by default, also called coefficient of determination, higher is better)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzrzcSkapTdO"
   },
   "source": [
    "### 1.2 What if there were missing values in the data?\n",
    "\n",
    "Holes in the data means holes in the patterns your machine learning model can learn.\n",
    "\n",
    "Many machine learning models don't work well or produce errors when they're used on datasets with missing values.\n",
    "\n",
    "A missing value can appear as a blank, as a `NaN` or something similar.\n",
    "\n",
    "There are two main options when dealing with missing values:\n",
    "\n",
    "1. **Fill them with some given or calculated value (imputation)** - For example, you might fill missing values of a numerical column with the mean of all the other values. The practice of calculating or figuring out how to fill missing values in a dataset is called **imputing**. For a great resource on imputing missing values, I'd recommend refering to the [Scikit-Learn user guide](https://scikit-learn.org/stable/modules/impute.html).\n",
    "2. **Remove them** - If a row or sample has missing values, you may opt to remove them from your dataset completely. However, this potentially results in using less data to build your model.\n",
    "\n",
    "> **Note:** Dealing with missing values differs from problem to problem, meaning there's no 100% best way to fill missing values across datasets and problem types. It will often take careful experimentation and practice to figure out the best way to deal with missing values in your own datasets.\n",
    "\n",
    "To practice dealing with missing values, let's import a version of the `car_sales` dataset with several missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "e_-twWbwpTdO",
    "outputId": "2d615dd7-5db5-4446-b0d4-3b537178de88"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Odometer (KM)</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>35431.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Blue</td>\n",
       "      <td>192714.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>84714.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>154365.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>181577.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14043.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Black</td>\n",
       "      <td>35820.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32042.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>White</td>\n",
       "      <td>155144.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>66604.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>31570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>215883.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Blue</td>\n",
       "      <td>248360.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12732.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Make Colour  Odometer (KM)  Doors    Price\n",
       "0     Honda  White        35431.0    4.0  15323.0\n",
       "1       BMW   Blue       192714.0    5.0  19943.0\n",
       "2     Honda  White        84714.0    4.0  28343.0\n",
       "3    Toyota  White       154365.0    4.0  13434.0\n",
       "4    Nissan   Blue       181577.0    3.0  14043.0\n",
       "..      ...    ...            ...    ...      ...\n",
       "995  Toyota  Black        35820.0    4.0  32042.0\n",
       "996     NaN  White       155144.0    3.0   5716.0\n",
       "997  Nissan   Blue        66604.0    4.0  31570.0\n",
       "998   Honda  White       215883.0    4.0   4001.0\n",
       "999  Toyota   Blue       248360.0    4.0  12732.0\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import car sales dataframe with missing values\n",
    "car_sales_missing = pd.read_csv(\"data/car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dja0yOcNpTdP"
   },
   "source": [
    "If you're dataset is large, it's likely you aren't going to go through it sample by sample to find the missing values.\n",
    "\n",
    "Luckily, pandas has a method called [`pd.DataFrame.isna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html) which is able to detect missing values.\n",
    "\n",
    "Let's try it on our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "oEy02fMDpTdP",
    "outputId": "5a156f6b-0f02-4c8a-e279-4e62504cd6b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             49\n",
       "Colour           50\n",
       "Odometer (KM)    50\n",
       "Doors            50\n",
       "Price            50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the sum of all missing values\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEH45qIUpTdQ"
   },
   "source": [
    "Hmm... seems there's about 50 or so missing values per column.\n",
    "\n",
    "How about we try and split the data into features and labels, then convert the categorical data to numbers, then split the data into training and test and then try and fit a model on it (just like we did before)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "bcld54KppTdQ",
    "outputId": "a332348f-4ae9-49a5-d2bc-ac93382e4b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing X values:\n",
      "Make             49\n",
      "Colour           50\n",
      "Odometer (KM)    50\n",
      "Doors            50\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create features\n",
    "X_missing = car_sales_missing.drop(\"Price\", axis=1)\n",
    "print(f\"Number of missing X values:\\n{X_missing.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "hHhJJGRypTdR",
    "outputId": "0ea8273b-f234-4797-bcdb-4be2dfb9c958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing y values: 50\n"
     ]
    }
   ],
   "source": [
    "# Create labels\n",
    "y_missing = car_sales_missing[\"Price\"]\n",
    "print(f\"Number of missing y values: {y_missing.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2stmFX4GpTdR"
   },
   "source": [
    "Now we can convert the categorical columns into one-hot encodings (just as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "8g4HXmSDpTdR",
    "outputId": "b02de118-e545-4259-efd9-66f708ffd315"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n",
       "        0.00000e+00, 3.54310e+04],\n",
       "       [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 1.92714e+05],\n",
       "       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n",
       "        0.00000e+00, 8.47140e+04],\n",
       "       ...,\n",
       "       [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 0.00000e+00,\n",
       "        0.00000e+00, 6.66040e+04],\n",
       "       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n",
       "        0.00000e+00, 2.15883e+05],\n",
       "       [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n",
       "        0.00000e+00, 2.48360e+05]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's convert the categorical columns to one hot encoded (code copied from above)\n",
    "# Turn the categories (Make and Colour) into numbers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                  one_hot,\n",
    "                                  categorical_features)],\n",
    "                                remainder=\"passthrough\",\n",
    "                                sparse_threshold=0) # return a sparse matrix or not\n",
    "\n",
    "transformed_X_missing = transformer.fit_transform(X_missing)\n",
    "transformed_X_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w15alXx9pTdS"
   },
   "source": [
    "Finally, let's split the missing data samples into train and test sets and then try to fit and score a model on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "BqNxtby6pTdS",
    "outputId": "5eb24643-8198-4d77-ab3f-3a0652f0e420"
   },
   "outputs": [],
   "source": [
    "# # Split data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(transformed_X_missing,\n",
    "#                                                     y_missing,\n",
    "#                                                     test_size=0.2)\n",
    "\n",
    "# # Fit and score a model\n",
    "# model = RandomForestRegressor()\n",
    "# model.fit(X_train, y_train)\n",
    "# model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzO_yWkTpTdS"
   },
   "source": [
    "Ahh... dam! Looks like the model we're trying to use doesn't work with missing values.\n",
    "\n",
    "When we try to fit it on a dataset with missing samples, Scikit-Learn produces the error:\n",
    "\n",
    "`ValueError: Input X contains NaN. RandomForestRegressor does not accept missing values encoded as NaN natively...`\n",
    "\n",
    "Looks like if we want to use `RandomForestRegressor`, we'll have to either fill or remove the missing values.\n",
    "\n",
    "<details class=\"tip\">\n",
    "    <summary><strong>Note:</strong> Scikit-Learn does have a\n",
    "        <a href=\"https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\">list of models which can handle NaNs or missing values directly</a>.\n",
    "    </summary>\n",
    "    <p>Such as,\n",
    "        <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html\"><code>sklearn.ensemble.HistGradientBoostingClassifier</code></a>\n",
    "        or\n",
    "        <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html\"><code>sklearn.ensemble.HistGradientBoostingRegressor</code></a>.\n",
    "    </p>\n",
    "    <p>As an experiment, you may want to try the following:</p>\n",
    "    <pre>\n",
    "<code>\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "\\# Try a model that can handle NaNs natively\n",
    "nan_model = HistGradientBoostingRegressor()\n",
    "nan_model.fit(X_train, y_train)\n",
    "nan_model.score(X_test, y_test)\n",
    "    </code>\n",
    "    </pre>\n",
    "</details>\n",
    "Let's see what values are missing again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "l0DxSawspTdT",
    "outputId": "b467feb4-a440-4bb7-b148-8f67cdf67c04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             49\n",
       "Colour           50\n",
       "Odometer (KM)    50\n",
       "Doors            50\n",
       "Price            50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0oiHhMWpTdT"
   },
   "source": [
    "How can fill (impute) or remove these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQkQpi5HpTdT"
   },
   "source": [
    "### 1.2.1 Fill missing data with pandas\n",
    "\n",
    "Let's see how we might fill missing values with pandas.\n",
    "\n",
    "For categorical values, one of the simplest ways is to fill the missing fields with the string `\"missing\"`.\n",
    "\n",
    "We could do this for the `Make` and `Colour` features.\n",
    "\n",
    "As for the `Doors` feature, we could use `\"missing\"` or we could fill it with the most common option of `4`.\n",
    "\n",
    "With the `Odometer (KM)` feature, we can use the mean value of all the other values in the column.\n",
    "\n",
    "And finally, for those samples which are missing a `Price` value, we can remove them (since `Price` is the target value, removing probably causes less harm than imputing, however, you could design an experiment to test this).\n",
    "\n",
    "In summary:\n",
    "\n",
    "| Column/Feature | Fill missing value with |\n",
    "| ----- | ----- |\n",
    "| `Make` | `\"missing\"` |\n",
    "| `Colour` | `\"missing\"` |\n",
    "| `Doors` | 4 (most common value) |\n",
    "| `Odometer (KM)` | mean of `Odometer (KM)` |\n",
    "| `Price` (target) | NA, remove samples missing `Price` |\n",
    "\n",
    "> **Note:** The practice of filling missing data with given or calculated values is called [**imputation**](https://scikit-learn.org/stable/modules/impute.html). And it's important to remember there's no perfect way to fill missing data (unless it's with data that should've actually been there in the first place). The methods we're using are only one of many. The techniques you use will depend heavily on your dataset. A good place to look would be searching for \"data imputation techniques\".\n",
    "\n",
    "Let's start with the `Make` column.\n",
    "\n",
    "We can use the pandas method [`fillna(value=\"missing\", inplace=True)`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html) to fill all the missing values with the string `\"missing\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "LTouauBbpTdU"
   },
   "outputs": [],
   "source": [
    "# Fill the missing values in the Make column\n",
    "car_sales_missing[\"Make\"] = car_sales_missing[\"Make\"].fillna(value=\"missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvuYsajcpTdU"
   },
   "source": [
    "And we can do the same with the `Colour` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "5ElV_v3kpTdU"
   },
   "outputs": [],
   "source": [
    "# Fill the Colour column\n",
    "car_sales_missing[\"Colour\"] = car_sales_missing[\"Colour\"].fillna(value=\"missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRuRDi7_pTdV"
   },
   "source": [
    "How many missing values do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "MBDQFCHIpTdV",
    "outputId": "d67bf830-674a-456c-8e77-cb96f3eb17ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make              0\n",
       "Colour            0\n",
       "Odometer (KM)    50\n",
       "Doors            50\n",
       "Price            50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBJl7NmmpTdV"
   },
   "source": [
    "Wonderful! We're making some progress.\n",
    "\n",
    "Now let's fill the `Doors` column with `4` (the most common value), this is the same as filling it with the [median](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.median.html) or [mode](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mode.html) of the `Doors` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "1ilNTbY8pTdW",
    "outputId": "28a9676b-fd4e-45b6-a6cb-f60529996333"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doors\n",
       "4.0    811\n",
       "5.0     75\n",
       "3.0     64\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most common value of the Doors column\n",
    "car_sales_missing[\"Doors\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "HFh35wa5pTdW"
   },
   "outputs": [],
   "source": [
    "# Fill the Doors column with the most common value\n",
    "car_sales_missing[\"Doors\"]=car_sales_missing[\"Doors\"].fillna(value=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VB4zHZ6YpTdW"
   },
   "source": [
    "Next, we'll fill the `Odometer (KM)` column with the mean value of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "ujwOefOZpTdX"
   },
   "outputs": [],
   "source": [
    "# Fill the Odometer (KM) column\n",
    "car_sales_missing[\"Odometer (KM)\"] = car_sales_missing[\"Odometer (KM)\"].fillna(value=car_sales_missing[\"Odometer (KM)\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yhDQ_-lpTdX"
   },
   "source": [
    "How many missing values do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "0PC_mq-RpTdY",
    "outputId": "ee00d4db-63b0-44ef-bd70-741a27128f2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make              0\n",
       "Colour            0\n",
       "Odometer (KM)     0\n",
       "Doors             0\n",
       "Price            50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of missing values\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbj2RxYRpTdY"
   },
   "source": [
    "Woohoo! That's looking a lot better.\n",
    "\n",
    "Finally, we can remove the rows which are missing the target value `Price`.\n",
    "\n",
    "> **Note:** Another option would be to impute the `Price` value with the mean or median or some other calculated value (such as by using similar cars to estimate the price), however, to keep things simple and prevent introducing too many fake labels to the data, we'll remove the samples missing a `Price` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "wBd6h92npTdY"
   },
   "outputs": [],
   "source": [
    "# Remove rows with missing Price labels\n",
    "car_sales_missing.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cZuy_LZpTdY"
   },
   "source": [
    "That should be no more missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ygEpWdn5pTdZ",
    "outputId": "40fa8ec9-5d3a-4e16-8118-91d0f8354940"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             0\n",
       "Colour           0\n",
       "Odometer (KM)    0\n",
       "Doors            0\n",
       "Price            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of missing values\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jqJ_KcPpTdZ"
   },
   "source": [
    "Since we removed samples missing a `Price` value, there's now less overall samples but none of them have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "GvYEHbzWpTdZ",
    "outputId": "fe7cc6bd-cb3e-4898-c370-c4c6dc8401bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "950"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of total samples (previously was 1000)\n",
    "len(car_sales_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiM_Z7sQpTdZ"
   },
   "source": [
    "Can we fit a model now?\n",
    "\n",
    "Let's try!\n",
    "\n",
    "First we'll create the features and labels.\n",
    "\n",
    "Then we'll convert categorical variables into numbers via one-hot encoding.\n",
    "\n",
    "Then we'll split the data into training and test sets just like before.\n",
    "\n",
    "Finally, we'll try to fit a `RandomForestRegressor()` model to the newly filled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "5FK0ya7GpTda",
    "outputId": "8846352e-d72c-40b3-b883-821890145ff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing X values:\n",
      "Make             0\n",
      "Colour           0\n",
      "Odometer (KM)    0\n",
      "Doors            0\n",
      "dtype: int64\n",
      "Number of missing y values: 0\n"
     ]
    }
   ],
   "source": [
    "# Create features\n",
    "X_missing = car_sales_missing.drop(\"Price\", axis=1)\n",
    "print(f\"Number of missing X values:\\n{X_missing.isna().sum()}\")\n",
    "\n",
    "# Create labels\n",
    "y_missing = car_sales_missing[\"Price\"]\n",
    "print(f\"Number of missing y values: {y_missing.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "pcTxADYtpTda",
    "outputId": "c28a66c7-a084-43c8-9d62-8a6a061f7a69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 3.54310e+04],\n",
       "       [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n",
       "        1.00000e+00, 1.92714e+05],\n",
       "       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 8.47140e+04],\n",
       "       ...,\n",
       "       [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 6.66040e+04],\n",
       "       [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 2.15883e+05],\n",
       "       [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00,\n",
       "        0.00000e+00, 2.48360e+05]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                  one_hot,\n",
    "                                  categorical_features)],\n",
    "                                remainder=\"passthrough\",\n",
    "                                sparse_threshold=0) # return a sparse matrix or not\n",
    "\n",
    "transformed_X_missing = transformer.fit_transform(X_missing)\n",
    "transformed_X_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "LcsIA1mIpTda",
    "outputId": "06bf5b0e-79ec-446b-972e-ab6fa5451537"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22011714008302485"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(transformed_X_missing,\n",
    "                                                    y_missing,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Fit and score a model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Arxrb5vSpTdb"
   },
   "source": [
    "Fantastic!!!\n",
    "\n",
    "Looks like filling the missing values with pandas worked!\n",
    "\n",
    "Our model can be fit to the data without issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUiJZsPvpTdb"
   },
   "source": [
    "### 1.2.2 Filling missing data and transforming categorical data with Scikit-Learn\n",
    "\n",
    "Now we've filled the missing columns using pandas functions, you might be thinking, \"Why pandas? I thought this was a Scikit-Learn introduction?\".\n",
    "\n",
    "Not to worry, Scikit-Learn provides a class called [`sklearn.impute.SimpleImputer()`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) which allows us to do a similar thing.\n",
    "\n",
    "`SimpleImputer()` transforms data by filling missing values with a given `strategy` parameter.\n",
    "\n",
    "And we can use it to fill the missing values in our DataFrame as above.\n",
    "\n",
    "At the moment, our dataframe has no mising values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Dra_KzEZpTdb",
    "outputId": "4f84b2e5-da8f-4da6-ae85-d68ae6158eeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             0\n",
       "Colour           0\n",
       "Odometer (KM)    0\n",
       "Doors            0\n",
       "Price            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y3D3IkvpTdc"
   },
   "source": [
    "Let's reimport it so it has missing values and we can fill them with Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "GwRjT8R5pTdc",
    "outputId": "05c4b72b-9af8-496a-8220-9907c004b999"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             49\n",
       "Colour           50\n",
       "Odometer (KM)    50\n",
       "Doors            50\n",
       "Price            50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reimport the DataFrame\n",
    "car_sales_missing = pd.read_csv(\"data/car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "272_TQrbpTdc"
   },
   "source": [
    "To begin, we'll remove the rows which are missing a `Price` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "JxqT5bZPpTdd"
   },
   "outputs": [],
   "source": [
    "# Drop the rows with missing in the Price column\n",
    "car_sales_missing.dropna(subset=[\"Price\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkSdnmsPpTdd"
   },
   "source": [
    "Now there are no rows missing a `Price` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "UFqgzL-0pTdd",
    "outputId": "06c44907-9e0a-4290-f9a3-9b8cfe2d379e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             47\n",
       "Colour           46\n",
       "Odometer (KM)    48\n",
       "Doors            47\n",
       "Price             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzIWiuxLpTde"
   },
   "source": [
    "Since we don't have to fill any `Price` values, let's split our data into features (`X`) and labels (`y`).\n",
    "\n",
    "We'll also split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Fc9MtfbapTde"
   },
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "X = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]\n",
    "\n",
    "# Split data into train and test\n",
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LilC9KJnpTde"
   },
   "source": [
    "> **Note:** We've split the data into train & test sets here first to perform filling missing values on them separately. This is best practice as the test set is supposed to emulate data the model has never seen before. For categorical variables, it's generally okay to fill values across the whole dataset. However, for numerical variables, you should **only fill values on the test set that have been computed from the training set**.\n",
    "\n",
    "Training and test sets created!\n",
    "\n",
    "Let's now setup a few instances of `SimpleImputer()` to fill various missing values.\n",
    "\n",
    "We'll use the following strategies and fill values:\n",
    "* For categorical columns (`Make`, `Colour`), `strategy=\"constant\"`, `fill_value=\"missing\"` (fill the missing samples with a consistent value of `\"missing\"`.\n",
    "* For the `Door` column, `strategy=\"constant\"`, `fill_value=4` (fill the missing samples with a consistent value of `4`).\n",
    "* For the numerical column (`Odometer (KM)`), `strategy=\"mean\"` (fill the missing samples with the mean of the target column).\n",
    "  * **Note:** There are more `strategy` and fill options in the [`SimpleImputer()` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "zH099TpSpTdf"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create categorical variable imputer\n",
    "cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\")\n",
    "\n",
    "# Create Door column imputer\n",
    "door_imputer = SimpleImputer(strategy=\"constant\", fill_value=4)\n",
    "\n",
    "# Create Odometer (KM) column imputer\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UC0Xwb3FpTdf"
   },
   "source": [
    "Imputers created!\n",
    "\n",
    "Now let's define which columns we'd like to impute on.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because we'll need these shortly (I'll explain in the next text cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "34PqI3X6pTdf"
   },
   "outputs": [],
   "source": [
    "# Define different column features\n",
    "categorical_features = [\"Make\", \"Colour\"]\n",
    "door_feature = [\"Doors\"]\n",
    "numerical_feature = [\"Odometer (KM)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zL8QWTDjpTdf"
   },
   "source": [
    "Columns defined!\n",
    "\n",
    "Now how might we transform our columns?\n",
    "\n",
    "Hint: we can use the [`sklearn.compose.ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) class from Scikit-Learn, in a similar way to what we did before to get our data to all numeric values.\n",
    "\n",
    "That's the reason we defined the columns we'd like to transform.\n",
    "\n",
    "So we can use the `ColumnTransformer()` class.\n",
    "\n",
    "`ColumnTransformer()` takes as input a list of tuples in the form `(name_of_transform, transformer_to_use, columns_to_transform)` specifying which columns to transform and how to transform them.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "imputer = ColumnTransformer([\n",
    "    (\"cat_imputer\", cat_imputer, categorical_features)\n",
    "])\n",
    "```\n",
    "\n",
    "In this case, the variables in the tuple are:\n",
    "* `name_of_transform` = `\"cat_imputer\"`\n",
    "* `transformer_to_use` = `cat_imputer` (the instance of `SimpleImputer()` we defined above)\n",
    "* `columns_to_transform` = `categorical_features` (the list of categorical features we defined above).\n",
    "\n",
    "Let's exapnd upon this by extending the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "joJ2IlKqpTdg"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Create series of column transforms to perform\n",
    "imputer = ColumnTransformer([\n",
    "    (\"cat_imputer\", cat_imputer, categorical_features),\n",
    "    (\"door_imputer\", door_imputer, door_feature),\n",
    "    (\"num_imputer\", num_imputer, numerical_feature)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbTJQfNNpTdg"
   },
   "source": [
    "Nice!\n",
    "\n",
    "The next step is to fit our `ColumnTransformer()` instance (`imputer`) to the training data and transform the testing data.\n",
    "\n",
    "In other words we want to:\n",
    "1. Learn the imputation values from the training set.\n",
    "2. Fill the missing values in the training set with the values learned in 1.\n",
    "3. Fill the missing values in the testing set with the values learned in 1.\n",
    "\n",
    "Why this way?\n",
    "\n",
    "In our case, we're not calculating many variables (except the mean of the `Odometer (KM)` column), however, remember that the test set should always remain as unseen data.\n",
    "\n",
    "So **when filling values in the test set, they should only be with values calculated or imputed from the training set**s.\n",
    "\n",
    "We can achieve steps 1 & 2 simultaneously with the [`ColumnTransformer.fit_transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer.fit_transform) method (`fit` = find the values to fill, `transform` = fill them).\n",
    "\n",
    "And then we can perform step 3 with the [`ColumnTransformer.transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer.transform) method (we only want to transform the test set, not learn different values to fill)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "goqrvkBbpTdg",
    "outputId": "0228ec22-9fcd-41d9-bca2-f5a152866388"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Honda', 'White', 4.0, 71934.0],\n",
       "       ['Toyota', 'Red', 4.0, 162665.0],\n",
       "       ['Honda', 'White', 4.0, 42844.0],\n",
       "       ...,\n",
       "       ['Toyota', 'White', 4.0, 196225.0],\n",
       "       ['Honda', 'Blue', 4.0, 133117.0],\n",
       "       ['Honda', 'missing', 4.0, 150582.0]], dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find values to fill and transform training data\n",
    "filled_X_train = imputer.fit_transform(X_train)\n",
    "\n",
    "# Fill values in to the test set with values learned from the training set\n",
    "filled_X_test = imputer.transform(X_test)\n",
    "\n",
    "# Check filled X_train\n",
    "filled_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSVhY707pTdh"
   },
   "source": [
    "Wonderful!\n",
    "\n",
    "Let's now turn our `filled_X_train` and `filled_X_test` arrays into DataFrames to inspect their missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "aYLxKdMUpTdh",
    "outputId": "ba4cacfb-e110-4004-f7f7-d6529526640b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             0\n",
       "Colour           0\n",
       "Doors            0\n",
       "Odometer (KM)    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get our transformed data array's back into DataFrame's\n",
    "filled_X_train_df = pd.DataFrame(filled_X_train,\n",
    "                                 columns=[\"Make\", \"Colour\", \"Doors\", \"Odometer (KM)\"])\n",
    "\n",
    "filled_X_test_df = pd.DataFrame(filled_X_test,\n",
    "                                columns=[\"Make\", \"Colour\", \"Doors\", \"Odometer (KM)\"])\n",
    "\n",
    "# Check missing data in training set\n",
    "filled_X_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_fXt7gBpTdh"
   },
   "source": [
    "And is there any missing data in the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "g26SmyV3pTdi",
    "outputId": "0637d96c-7006-4dd8-faae-be9680c6246b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             0\n",
       "Colour           0\n",
       "Doors            0\n",
       "Odometer (KM)    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing data in the testing set\n",
    "filled_X_test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKqM5jnCpTdi"
   },
   "source": [
    "What about the original?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "J7t1YsXypTdi",
    "outputId": "637d285b-e7aa-4256-a32f-629be85c97dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             47\n",
       "Colour           46\n",
       "Odometer (KM)    48\n",
       "Doors            47\n",
       "Price             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see the original... still missing values\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68MbI7OOpTdj"
   },
   "source": [
    "Perfect!\n",
    "\n",
    "No more missing values!\n",
    "\n",
    "But wait...\n",
    "\n",
    "Is our data all numerical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "EPhdn6WNpTdj",
    "outputId": "34f47f41-7ddc-41cd-ace4-ba07e90db28d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Odometer (KM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>4.0</td>\n",
       "      <td>71934.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Red</td>\n",
       "      <td>4.0</td>\n",
       "      <td>162665.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>4.0</td>\n",
       "      <td>42844.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>4.0</td>\n",
       "      <td>195829.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>4.0</td>\n",
       "      <td>219217.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Make Colour Doors Odometer (KM)\n",
       "0   Honda  White   4.0       71934.0\n",
       "1  Toyota    Red   4.0      162665.0\n",
       "2   Honda  White   4.0       42844.0\n",
       "3   Honda  White   4.0      195829.0\n",
       "4   Honda   Blue   4.0      219217.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filled_X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8-mUp9npTdj"
   },
   "source": [
    "Ahh... looks like our `Make` and `Colour` columns are still strings.\n",
    "\n",
    "Let's one-hot encode them along with the `Doors` column to make sure they're numerical, just as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "pKLnAxVupTdj",
    "outputId": "e72521a4-fe64-4749-b5f1-9ceac921eabd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, ..., 1.0, 0.0, 71934.0],\n",
       "       [0.0, 0.0, 0.0, ..., 1.0, 0.0, 162665.0],\n",
       "       [0.0, 1.0, 0.0, ..., 1.0, 0.0, 42844.0],\n",
       "       ...,\n",
       "       [0.0, 0.0, 0.0, ..., 1.0, 0.0, 196225.0],\n",
       "       [0.0, 1.0, 0.0, ..., 1.0, 0.0, 133117.0],\n",
       "       [0.0, 1.0, 0.0, ..., 1.0, 0.0, 150582.0]], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's one hot encode the features with the same code as before\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                  one_hot,\n",
    "                                  categorical_features)],\n",
    "                                remainder=\"passthrough\",\n",
    "                                sparse_threshold=0) # return a sparse matrix or not\n",
    "\n",
    "# Fill train and test values separately\n",
    "transformed_X_train = transformer.fit_transform(filled_X_train_df)\n",
    "transformed_X_test = transformer.transform(filled_X_test_df)\n",
    "\n",
    "# Check transformed and filled X_train\n",
    "transformed_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-Vjk3r-pTdk"
   },
   "source": [
    "Nice!\n",
    "\n",
    "Now our data is:\n",
    "1. All numerical\n",
    "2. No missing values\n",
    "\n",
    "Let's try and fit a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "iydfyVZIpTdk",
    "outputId": "62439145-2171-4e86-ef13-66c50e57fe86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21229043336119102"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we've transformed X, let's see if we can fit a model\n",
    "np.random.seed(42)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Make sure to use the transformed data (filled and one-hot encoded X data)\n",
    "model.fit(transformed_X_train, y_train)\n",
    "model.score(transformed_X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBklWnSgpTdl"
   },
   "source": [
    "You might have noticed this result is slightly different to before.\n",
    "\n",
    "Why do you think this is?\n",
    "\n",
    "It's because we've created our training and testing sets differently.\n",
    "\n",
    "We split the data into training and test sets *before* filling the missing values.\n",
    "\n",
    "Previously, we did the reverse, filled missing values *before* splitting the data into training and test sets.\n",
    "\n",
    "Doing this can lead to information from the training set leaking into the testing set.\n",
    "\n",
    "Remember, one of the most important concepts in machine learning is making sure your model doesn't see *any* testing data before evaluation.\n",
    "\n",
    "We'll keep practicing but for now, some of the main takeaways are:\n",
    "* Keep your training and test sets separate.\n",
    "* Most datasets you come across won't be in a form ready to immediately start using them with machine learning models. And some may take more preparation than others to get ready to use.\n",
    "* For most machine learning models, your data has to be numerical. This will involve converting whatever you're working with into numbers. This process is often referred to as **feature engineering** or **feature encoding**.\n",
    "* Some machine learning models aren't compatible with missing data. The process of filling missing data is referred to as **data imputation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dg7WijnpTdl"
   },
   "source": [
    "## 2. Choosing the right estimator/algorithm for your problem\n",
    "\n",
    "Once you've got your data ready, the next step is to choose an appropriate machine learning algorithm or model to find patterns in your data.\n",
    "\n",
    "Some things to note:\n",
    "* Scikit-Learn refers to machine learning models and algorithms as estimators.\n",
    "* **Classification problem** - predicting a category (heart disease or not).\n",
    "    * Sometimes you'll see `clf` (short for classifier) used as a classification estimator instance's variable name.\n",
    "* **Regression problem** - predicting a number (selling price of a car).\n",
    "* **Unsupervised problem (data with no labels)** - clustering (grouping unlabelled samples with other similar unlabelled samples).\n",
    "\n",
    "If you know what kind of problem you're working with, one of the next places you should look at is the [Scikit-Learn algorithm cheatsheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n",
    "\n",
    "This cheatsheet gives you a bit of an insight into the algorithm you might want to use for the problem you're working on.\n",
    "\n",
    "It's important to remember, you don't have to explicitly know what each algorithm is doing on the inside to start using them.\n",
    "\n",
    "If you start to apply different algorithms but they don't seem to be working (not performing as well as you'd like), that's when you'd start to look deeper into each one.\n",
    "\n",
    "Let's check out the cheatsheet and follow it for some of the problems we're working on.\n",
    "\n",
    "<img src=\"images/sklearn-ml-map.png\" width=700/>\n",
    "\n",
    "You can see it's split into four main categories. Regression, classification, clustering and dimensionality reduction. Each has their own different purpose but the Scikit-Learn team has designed the library so the workflows for each are relatively similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytCjaGlppTdl"
   },
   "source": [
    "### 2.1 Picking a machine learning model for a regression problem\n",
    "\n",
    "Let's start with a regression problem (trying to predict a number). We'll use the [California Housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) built into Scikit-Learn's `datasets` module.\n",
    "\n",
    "The goal of the California Housing dataset is to predict a given district's median house value (in hundreds of thousands of dollars) on things like the age of the home, the number of rooms, the number of bedrooms, number of people living the home and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "gc6SzmFspTdl"
   },
   "outputs": [],
   "source": [
    "# Get California Housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing; # gets downloaded as dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zw14HacpTdm"
   },
   "source": [
    "Since it's in a dictionary, let's turn it into a DataFrame so we can inspect it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "WGrMxAhbpTdm",
    "outputId": "e332a60c-f13d-47dd-873b-6036b544542a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df = pd.DataFrame(housing[\"data\"], columns=housing[\"feature_names\"])\n",
    "housing_df[\"target\"] = pd.Series(housing[\"target\"])\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "XHFlhGAApTdm",
    "outputId": "e3733c81-e9be-426a-cd72-5c7c36055bbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20640"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many samples?\n",
    "len(housing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouTkQxbypTdn"
   },
   "source": [
    "Beautiful, our goal here is to use the feature columns, such as:\n",
    "* `MedInc` - median income in block group\n",
    "* `HouseAge` - median house age in block group\n",
    "* `AveRooms` - average number of rooms per household\n",
    "* `AveBedrms` - average number of bedrooms per household\n",
    "\n",
    "To predict the `target` column which expresses the median house value for specific California districts in hundreds of thousands of dollars (e.g. 4.526 = $452,600).\n",
    "\n",
    "In essence, each row is a different district in California (the data) and we're trying to build a model to predict the median house value in that district (the target/label) given a series of attributes about the houses in that district.\n",
    "\n",
    "Since we have data and labels, this is a supervised learning problem.\n",
    "\n",
    "And since we're trying to predict a number, it's a regression problem.\n",
    "\n",
    "Knowing these two things, how do they line up on the Scikit-Learn machine learning algorithm cheat-sheet?\n",
    "\n",
    "<img src=\"https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/images/sklearn-ml-map-cheatsheet-california-housing-ridge.png?raw=1\" width=700/>\n",
    "\n",
    "Following the map through, knowing what we know, it suggests we try [`RidgeRegression`](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression). Let's chek it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "sO8iymscpTdn",
    "outputId": "8ed98869-8add-4eca-875e-99a03cda8686"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5758549611440126"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Ridge model class from the linear_model module\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Institate and fit the model (on the training set)\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Check the score of the model (on the test set)\n",
    "# The default score() metirc of regression aglorithms is R^2\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "490Tb6MDpTdo"
   },
   "source": [
    "What if `RidgeRegression` didn't work? Or what if we wanted to improve our results?\n",
    "\n",
    "<img src=\"https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/images/sklearn-ml-map-cheatsheet-california-housing-ensemble.png?raw=1\" width=700/>\n",
    "\n",
    "Following the diagram, the next step would be to try [`EnsembleRegressors`](https://scikit-learn.org/stable/modules/ensemble.html).\n",
    "\n",
    "Ensemble is another word for multiple models put together to make a decision.\n",
    "\n",
    "One of the most common and useful ensemble methods is the [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest). Known for its fast training and prediction times and adaptibility to different problems.\n",
    "\n",
    "The basic premise of the Random Forest is to combine a number of different decision trees, each one random from the other and make a prediction on a sample by averaging the result of each decision tree.\n",
    "\n",
    "An in-depth discussion of the Random Forest algorithm is beyond the scope of this notebook but if you're interested in learning more, [An Implementation and Explanation of the Random Forest in Python](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76) by Will Koehrsen is a great read.\n",
    "\n",
    "Since we're working with regression, we'll use Scikit-Learn's [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).\n",
    "\n",
    "We can use the exact same workflow as above. Except for changing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "Zn2IhtkopTdo",
    "outputId": "0ddb3591-9bf9-45f4-b9a2-ca4d20faaa69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8065734772187598"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the RandomForestRegressor model class from the ensemble module\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Institate and fit the model (on the training set)\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Check the score of the model (on the test set)\n",
    "# The default score metirc of regression aglorithms is R^2\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGdN8pAgpTdp"
   },
   "source": [
    "Woah!\n",
    "\n",
    "We get a good boost in score on the test set by changing the model.\n",
    "\n",
    "This is another incredibly important concept in machine learning, if at first something doesn't achieve what you'd like, *experiment, experiment, experiment!*\n",
    "\n",
    "At first, the Scikit-Learn algorithm diagram can seem confusing.\n",
    "\n",
    "But once you get a little practice applying different models to different problems, you'll start to pick up which sorts of algorithms do better with different types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as77DttCpTdp"
   },
   "source": [
    "### 2.2 Picking a machine learning model for a classification problem\n",
    "\n",
    "Now, let's check out the choosing process for a classification problem.\n",
    "\n",
    "Say you were trying to predict whether or not a patient had heart disease based on their medical records.\n",
    "\n",
    "The dataset in `data/heart-disease.csv` contains data for just that problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "XKN9f4dMpTdp",
    "outputId": "d9045f3e-8606-4a95-d2dc-adfc84688be3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_disease = pd.read_csv(\"data/heart-disease.csv\")\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "rHOvNzJUpTdq",
    "outputId": "a5c5c984-e849-4396-a53b-7607277f5a34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many samples are there?\n",
    "len(heart_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeg6O6pLpTdq"
   },
   "source": [
    "Similar to the California Housing dataset, here we want to use all of the available data to predict the target column (1 for if a patient has heart disease and 0 for if they don't).\n",
    "\n",
    "So what do we know?\n",
    "\n",
    "We've got 303 samples (1 row = 1 sample) and we're trying to predict whether or not a patient has heart disease.\n",
    "\n",
    "Because we're trying to predict whether each sample is one thing or another, we've got a classification problem.\n",
    "\n",
    "Let's see how it lines up with our [Scikit-Learn algorithm cheat-sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n",
    "\n",
    "<img src=\"https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/images/sklearn-ml-map-cheatsheet-heart-disease-linear-svc.png?raw=1\" width=700/>\n",
    "\n",
    "Following the cheat-sheet we end up at [`LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) which stands for Linear Support Vector Classifier. Let's try it on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "sVBXmpW2pTdr",
    "outputId": "c155dda7-6d9b-4f53-fafc-7107770087b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8688524590163934"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import LinearSVC from the svm module\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the data into X (features/data) and y (target/labels)\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate and fit the model (on the training set)\n",
    "clf = LinearSVC(max_iter=1000, # iterations on the data, 1000 is the default\n",
    "                dual=\"auto\") # dual=\"auto\" chooses best parameters for the model automatically\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Check the score of the model (on the test set)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oqXXZz7pTdr"
   },
   "source": [
    "Straight out of the box (with no tuning or improvements) our model achieves over 85% accruacy!\n",
    "\n",
    "Although this is a sensational result to begin with, let's check out the diagram and see what other models we might use.\n",
    "\n",
    "<img src=\"https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/images/sklearn-ml-map-cheatsheet-heart-disease-ensemble.png?raw=1\" width=700/>\n",
    "\n",
    "Following the path (and skipping a few, don't worry, we'll get to this) we come up to [`EnsembleMethods`](https://scikit-learn.org/stable/modules/ensemble.html) again.\n",
    "\n",
    "Except this time, we'll be looking at ensemble classifiers instead of regressors.\n",
    "\n",
    "Remember our [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) from above?\n",
    "\n",
    "We'll it has a dance partner, [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) which is an ensemble based machine model learning model for classification.\n",
    "\n",
    "You might be able to guess what we can use it for (hint: classification problems).\n",
    "\n",
    "Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "QErmiIJ3pTds",
    "outputId": "f72d992a-300b-4b6d-a736-616e15ce6a2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the RandomForestClassifier model class from the ensemble module\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the data into X (features/data) and y (target/labels)\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate and fit the model (on the training set)\n",
    "clf = RandomForestClassifier(n_estimators=100) # 100 is the default, but you could try 1000 and see what happens\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Check the score of the model (on the test set)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9wYSP7xpTds"
   },
   "source": [
    "Hmm, it seems like the default settings of `RandomForestClassifier` aren't performing as well as `LinearSVC`.\r\n",
    "\r\n",
    "Apart from trying a different classification model, we could start experimenting to enhance these models through [hyperparameter tuning](http://scikit-learn.org/stable/modules/grid_search.html).\r\n",
    "\r\n",
    "Hyperparameter tuning is a fancy term for adjusting certain settings on a model to try and improve its performance.\r\n",
    "\r\n",
    "This typically occurs once we've found a reasonable baseline model that we want to enhance.\r\n",
    "\r\n",
    "In this case, we could take either the `RandomForestClassifier` or the `LinearSVC` and attempt to improve it with hyperparameter tuning (which we'll delve into later).\r\n",
    "\r\n",
    "For instance, we could take the `n_estimators` parameter (the number of trees in the forest) of `RandomForestClassifier` and change it from the default `100` to `1000` to observe te impact.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etLysJF6pTdt"
   },
   "source": [
    "### Exploring Other Models\r\n",
    "\r\n",
    "As we continue our journey in machine learning, you might be wondering about the other models we haven't covered yet.\r\n",
    "\r\n",
    "Why did we skip them?\r\n",
    "\r\n",
    "Firstly, time is a factor. Learning about every single model in depth would take quite a while, and we want to focus on building a solid foundation efficiently. \r\n",
    "\r\n",
    "But fear not! The models we've explored so far are just the beginning of the machine learning landscape. There are many other algorithms out there waiting to be discovered and understood.\r\n",
    "\r\n",
    "One important concept to keep in mind is the effectiveness of ensemble methods. Here's a quick tip for navigating the world of machine learning models:\r\n",
    "\r\n",
    "- **Structured Data**: If you're working with structured data, like tables or spreadsheets, ensemble methods such as Random Forests are often a great choice. They excel at capturing complex patterns in data and making accurate predictions.\r\n",
    "\r\n",
    "- **Unstructured Data**: On the other hand, if you're dealing with unstructured data, like text or images, deep learning or transfer learning might be more suitable. These techniques are designed to handle the intricacies of unstructured data and can extract meaningful insights from them.\r\n",
    "\r\n",
    "For now, we're focusing on structured data, which is why we've been exploring the Random Forest model. But remember, this is just the tip of the iceberg!\r\n",
    "\r\n",
    "If you're curious to learn more about Random Forests and why they're considered the workhorse of machine learning, I encourage you to check out these resources:\r\n",
    "\r\n",
    "- [Random Forest Wikipedia](https://en.wikipedia.org/wiki/Random_forest)\r\n",
    "- [An Implementation and Explanation of the Random Forest in Python](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76) by Will Koehrsen\r\n",
    "\r\n",
    "Keep exploring, keep learning, and remember that every step you take brings you closer to mastering machine learning!\r\n",
    "y Will Koehrsen\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZ4i7YUTpTdt"
   },
   "source": [
    "### Embracing Experimentation: Learning Through Trial and Error\r\n",
    "\r\n",
    "In the world of machine learning, the Scikit-Learn API offers a consistent framework across different models. Once you've grasped the workflow with one model, transitioning to others becomes much smoother.\r\n",
    "\r\n",
    "As a student aspiring to become proficient in machine learning, it's essential to embrace experimentation. Trying out different models listed on the cheat-sheet is not just about finding the \"right\" one; it's about understanding their nuances and learning from the process.\r\n",
    "\r\n",
    "Remember, as you embark on your journey of exploration, each experiment is an opportunity for growth. Embrace the iterative nature of machine learning, and don't be afraid to learn from both successes and failures. After all, it's through experimentation that we truly deepen our understanding of the ield.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-HR1Du9pTdt"
   },
   "source": [
    "## 3. Fit the model to data and using it to make predictions\n",
    "\n",
    "Now you've chosen a model, the next step is to have it learn from the data so it can be used for predictions in the future.\n",
    "\n",
    "If you've followed through, you've seen a few examples of this already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nix07yV-pTdu"
   },
   "source": [
    "### 3.1 Fitting a model to data\n",
    "\n",
    "In Scikit-Learn, the process of having a machine learning model learn patterns from a dataset involves calling the `fit()` method and passing it data, such as, `fit(X, y)`.\n",
    "\n",
    "Where `X` is a feature array and `y` is a target array.\n",
    "\n",
    "Other names for `X` include:\n",
    "* Data\n",
    "* Feature variables\n",
    "* Features\n",
    "\n",
    "Other names for `y` include:\n",
    "* Labels\n",
    "* Target variable\n",
    "\n",
    "For supervised learning there is usually an `X` and `y`.\n",
    "\n",
    "For unsupervised learning, there's no `y` (no labels).\n",
    "\n",
    "Let's revisit the example of using patient data (`X`) to predict whether or not they have heart disease (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "NimI6vjfpTdu",
    "outputId": "5b14dee7-09a5-4207-a5fe-3bcdb3ffcde7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the RandomForestClassifier model class from the ensemble module\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the data into X (features/data) and y (target/labels)\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the model (on the training set)\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Call the fit method on the model and pass it training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Check the score of the model (on the test set)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2dE3pgkpTdu"
   },
   "source": [
    "What's happening here?\n",
    "\n",
    "Calling the `fit()` method will cause the machine learning algorithm to attempt to find patterns between `X` and `y`. Or if there's no `y`, it'll only find the patterns within `X`.\n",
    "\n",
    "Let's see `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "rratMhqYpTdu",
    "outputId": "e36d8fe8-9f52-4727-cd11-7f6f3b67a09e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  \n",
       "0   0     1  \n",
       "1   0     2  \n",
       "2   0     2  \n",
       "3   0     2  \n",
       "4   0     2  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXiy6o2dpTdv"
   },
   "source": [
    "And `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "G1BPLKaIpTdv",
    "outputId": "a98f8995-3a47-4300-f51e-c4d641b34ede"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEK5qnxrpTdv"
   },
   "source": [
    "When I pass `X` and `y` to the `fit()` method, the model goes through all the examples in `X` (the data) and learns their corresponding `y` (the labels).\r\n",
    "\r\n",
    "How the model does this varies depending on the specific model I'm using.\r\n",
    "\r\n",
    "Explaining the details of how each model works would require an entire textbook. For now, I can imagine it similarly to how I would figure out patterns if I had enough time. I would look at the feature variables in `X`, such as `age`, `sex`, `chol` (cholesterol), and see which values lead to the labels in `y`—where `1` indicates heart disease and `0` indicates no heart disease.\r\n",
    "\r\n",
    "This basic concept of learning patterns from data seems to be common across all of machine learning.\r\n",
    "\r\n",
    "**During training (finding patterns in data):**\r\n",
    "\r\n",
    "A machine learning algorithm examines a dataset, finds patterns, tries to use those patterns to make predictions, and adjusts itself as best it can based on the available data and labels. It stores these patterns for future use.\r\n",
    "\r\n",
    "**During testing or in production (using learned patterns):**\r\n",
    "\r\n",
    "A machine learning algorithm uses the patterns it previously learned from a dataset to make predictions on new, unseen data.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-fkBX5epTdv"
   },
   "source": [
    "### 3.2 Making predictions using a machine learning model\n",
    "Now we've got a trained model, one which has hoepfully learned patterns in the data, you'll want to use it to make predictions.\n",
    "\n",
    "Scikit-Learn enables this in several ways.\n",
    "\n",
    "Two of the most common and useful are [`predict()`](https://github.com/scikit-learn/scikit-learn/blob/5f3c3f037/sklearn/multiclass.py#L299) and [`predict_proba()`](https://github.com/scikit-learn/scikit-learn/blob/5f3c3f037/sklearn/linear_model/_logistic.py#L1617).\n",
    "\n",
    "Let's see them in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "uDEmG83spTdw",
    "outputId": "3dce50ef-f60d-41a8-9a8c-9535b7311334"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a trained model to make predictions\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3mU_hbJpTdw"
   },
   "source": [
    "Given data in the form of `X`, the `predict()` function returns labels in the form of `y`.\n",
    "\n",
    "> **Note:** For the `predict()` function to work, it must be passed `X` (data) in the same format the model was trained on. For example, if a model was trained on 10 features formatted in a certain way, predictions should be made on data with 10 features fortmatted in a certain way. Anything different and it will return an error.\n",
    "\n",
    "It's standard practice to save these predictions to a variable named something like `y_preds` for later comparison to `y_test` or `y_true` (usually same as `y_test` just another name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "Aa6akBwupTdw",
    "outputId": "48f64008-6d81-4627-e899-6683cc77a427"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare predictions to truth\n",
    "y_preds = clf.predict(X_test)\n",
    "np.mean(y_preds == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdWSI128pTdx"
   },
   "source": [
    "Another way evaluating predictions (comparing them to the truth labels) is with Scikit-Learn's [`sklearn.metrics` module](http://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "\n",
    "Inside, you'll find method such as [`accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html), which is the default evaluation metric for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "gdbeGHZapTdx",
    "outputId": "a9e50306-e9df-4c7a-e3df-48d188ced408"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTbPjpk1pTdx"
   },
   "source": [
    "`predict_proba()` returns the probabilities (proba is short for probability) of a classification label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "SftFgDYdpTdx",
    "outputId": "5b8bf3f1-21f3-41ee-844a-778a500df4fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89, 0.11],\n",
       "       [0.49, 0.51],\n",
       "       [0.43, 0.57],\n",
       "       [0.84, 0.16],\n",
       "       [0.18, 0.82]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return probabilities rather than labels\n",
    "clf.predict_proba(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsFlUCTPpTdy"
   },
   "source": [
    "Let's see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "dZojfn9apTd2",
    "outputId": "bd941597-d9c9-4b0c-a77c-a3720ba7b79d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return labels\n",
    "clf.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_kG5RT5pTd3"
   },
   "source": [
    "`predict_proba()` returns an array of five arrays each containing two values.\n",
    "\n",
    "Each number is the probability of a label given a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "Ppx6ycI_pTd3",
    "outputId": "bf939543-d2fe-4f1d-fe76-446696630dc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89, 0.11]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find prediction probabilities for 1 sample\n",
    "clf.predict_proba(X_test[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NFupStIpTd3"
   },
   "source": [
    "This output means for the sample `X_test[:1]`, the model is predicting label 0 (index 0) with a probability score of 0.9.\n",
    "\n",
    "Because the highest probability score is at index `0` (and it's over 0.5), when using `predict()`, a label of `0` is assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "v7T7W7aJpTd3",
    "outputId": "75831d57-e6f0-48b8-8d95-cac33d8fb6d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the label for 1 sample\n",
    "clf.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaukJ7UQpTd4"
   },
   "source": [
    "Where does 0.5 come from?\n",
    "\n",
    "Because our problem is a binary classification task (heart disease or not heart disease), predicting a label with 0.5 probability every time would be the same as a coin toss (guessing 50/50 every time).\n",
    "\n",
    "Therefore, once the prediction probability of a sample passes 0.5 for a certain label, it's assigned that label.\n",
    "\n",
    "`predict()` can also be used for regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "sM0JJKXZpTd4"
   },
   "outputs": [],
   "source": [
    "# Import the RandomForestRegressor model class from the ensemble module\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Institate and fit the model (on the training set)\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1I_1KP_HpTd5"
   },
   "source": [
    "Now we can evaluate our regression model by using [`sklearn.metrics.mean_absolute_error`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) which returns the average error across all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "NsPje0gjpTd5",
    "outputId": "8d0ae43a-8138-4e24-cff7-c6b26e9bda8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32659871732073664"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the predictions to the truth\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test == y_preds) #it only works for classfication problems not for regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression `np.mean(y_test == y_preds)` calculates the proportion of correct predictions (i.e., the accuracy) by comparing the true labels `y_test` with the predicted labels `y_preds`. This method is specifically useful for classification problems.\n",
    "`y_test == y_preds` evaluates to an array of boolean values, such as `[True, True, True, False, False]`, indicating where the predictions match the true labels. Taking the mean of this boolean array gives the proportion of correct predictions (accuracy), because in Python, True is treated as `1` and False as `0`.\n",
    "\n",
    "In regression problems, `y_test and y_preds` are arrays of continuous values, not discrete labels. Using `y_test == y_preds` directly in regression does not make sense because exact matches between continuous values are rare and not meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuW88A5-pTd5"
   },
   "source": [
    "Now we've seen how to get a model how to find patterns in data using the `fit()` function and make predictions using what its learned using the `predict()` and `predict_proba()` functions, it's time to evaluate those predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pBjPsjdpTd5"
   },
   "source": [
    "## 4. Evaluating a model\n",
    "\n",
    "Once you've trained a model, you'll want a way to measure how trustworthy its predictions are.\n",
    "\n",
    "Across the board, the main idea of evaluating a model is to **compare the model's predictions to what they should've ideally been** (the truth labels).\n",
    "\n",
    "Scikit-Learn implements 3 different methods of evaluating models.\n",
    "\n",
    "1. The `score()` method. Calling `score()` on a model instance will return a metric assosciated with the type of model you're using. The metric depends on which model you're using.\n",
    "2. The `scoring` parameter. This parameter can be passed to methods such as [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) or [`GridSearchCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to tell Scikit-Learn to use a specific type of scoring metric.\n",
    "3. Problem-specific metric functions available in [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics). Similar to how the `scoring` parameter can be passed different scoring functions, Scikit-Learn implements these as stand alone functions.\n",
    "\n",
    "The scoring function you use will also depend on the problem you're working on.\n",
    "\n",
    "Classification problems have different evaluation metrics and scoring functions to regression problems.\n",
    "\n",
    "Let's look at some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRXXV123pTd5"
   },
   "source": [
    "### 4.1 General model evaluation with `score()`\n",
    "\n",
    "If we bring down the code from our previous classification problem (building a classifier to predict whether or not someone has heart disease based on their medical records).\n",
    "\n",
    "We can see the `score()` method come into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "DxKHNPwcpTd6"
   },
   "outputs": [],
   "source": [
    "# Import the RandomForestClassifier model class from the ensemble module\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the data into X (features/data) and y (target/labels)\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the model (on the training set)\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Call the fit method on the model and pass it training data\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARmYDVtBpTd6"
   },
   "source": [
    "Once the model has been fit on the training data (`X_train`, `y_train`), we can call the `score()` method on it and evaluate our model on the test data, data the model has never seen before (`X_test`, `y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "0hDaGnrKpTd6",
    "outputId": "dc3c8e1d-4024-4e1a-c1f8-044cdc3f672b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the score of the model (on the test set)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vgYvVW7pTd6"
   },
   "source": [
    "Each model in Scikit-Learn implements a default metric for `score()` which is suitable for the problem.\n",
    "\n",
    "For example:\n",
    "* Classifier models generally use [`metrics.accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) as the default `score()` metric.\n",
    "* Regression models generally use [`metrics.r2_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) as the default `score()` metric.\n",
    "* There many more [classification](https://scikit-learn.org/stable/modules/classes.html#classification-metrics) and [regression](https://scikit-learn.org/stable/modules/classes.html#regression-metrics) specific metrics implemented in `sklearn.metrics`.\n",
    "\n",
    "Because `clf` is an instance of `RandomForestClassifier`, the `score()` method uses mean accuracy as its score method.\n",
    "\n",
    "You can find this by pressing **SHIFT + TAB** (inside a Jupyter Notebook, may be different elsewhere) within the brackets of `score()` when called on a model instance.\n",
    "\n",
    "Behind the scenes, `score()` makes predictions on `X_test` using the trained model and then compares those predictions to the actual labels `y_test`.\n",
    "\n",
    "A classification model which predicts everything 100% correct would receive an accuracy score of 1.0 (or 100%).\n",
    "\n",
    "Our model doesn't get everything correct, but at ~85% accuracy (0.85 * 100), it's still far better than guessing.\n",
    "\n",
    "Let's do the same but with the regression code from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r26jVsE8pTd6"
   },
   "outputs": [],
   "source": [
    "# Import the RandomForestRegressor model class from the ensemble module\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Institate and fit the model (on the training set)\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PRfNPT8pTd7"
   },
   "source": [
    "Due to the consistent design of the Scikit-Learn library, we can call the same `score()` method on `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpQfGwffpTd7",
    "outputId": "faebc31b-a0ae-426f-c22f-1cde5636d3f3"
   },
   "outputs": [],
   "source": [
    "# Check the score of the model (on the test set)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR5LB-lopTd7"
   },
   "source": [
    "Here, `model` is an instance of [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).\n",
    "\n",
    "And since it's a regression model, the default metric built into `score()` is the coefficient of determination or R^2 (pronounced R-sqaured).\n",
    "\n",
    "Remember, you can find this by pressing **SHIFT + TAB** within the brackets of `score()` when called on a model instance.\n",
    "\n",
    "The best possible value here is 1.0, this means the model predicts the target regression values exactly.\n",
    "\n",
    "Calling the `score()` method on any model instance and passing it test data is a good quick way to see how your model is going.\n",
    "\n",
    "However, when you get further into a problem, it's likely you'll want to start using more powerful metrics to evaluate your models performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuYlQDtcpTd7"
   },
   "source": [
    "### 4.2 Evaluating your models using the `scoring` parameter\n",
    "\n",
    "The next step up from using `score()` is to use a custom `scoring` parameter with [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) or [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "As you may have guessed, the `scoring` parameter you set will be different depending on the problem you're working on.\n",
    "\n",
    "We'll see some specific examples of different parameters in a moment but first let's check out `cross_val_score()`.\n",
    "\n",
    "To do so, we'll copy the heart disease classification code from above and then add another line at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5E9GCuApTd7"
   },
   "outputs": [],
   "source": [
    "# Import cross_val_score from the model_selection module\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Import the RandomForestClassifier model class from the ensemble module\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the data into X (features/data) and y (target/labels)\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the model (on the training set)\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Call the fit method on the model and pass it training data\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu2_ztL0pTd7"
   },
   "source": [
    "Using `cross_val_score()` is slightly different to `score()`.\n",
    "\n",
    "Let's see a code example first and then we'll go through the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Chafr-C8pTd7",
    "outputId": "5d64fe74-ea90-4a09-ad0e-aefcc3824599"
   },
   "outputs": [],
   "source": [
    "# Using score()\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJef-TL4pTd8",
    "outputId": "73600b04-977f-4686-a389-a87a15880470"
   },
   "outputs": [],
   "source": [
    "# Using cross_val_score()\n",
    "cross_val_score(clf, X, y, cv=5) # cv = number of splits to test (5 by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfEjdD1qpTd8"
   },
   "source": [
    "What's happening here?\n",
    "\n",
    "The first difference you might notice is `cross_val_score()` returns an array where as `score()` only returns a single number.\n",
    "\n",
    "`cross_val_score()` returns an array because of a parameter called `cv`, which stands for cross-validation.\n",
    "\n",
    "When `cv` isn't set, `cross_val_score()` will return an array of 5 numbers by default (`cv=None` is the same as setting `cv=5`).\n",
    "\n",
    "Remember, you can see the parameters of a function using **SHIFT + TAB** (inside a Jupyter Notebook) from within the brackets.\n",
    "\n",
    "But wait, you might be thinking, what even is cross-validation?\n",
    "\n",
    "A visual might be able to help.\n",
    "\n",
    "<img src='images/sklearn-cross-validation.png' width=600/>\n",
    "\n",
    "We've dealt with Figure 1.0 before using `score(X_test, y_test)`.\n",
    "\n",
    "But looking deeper into this, if a model is trained using the training data or 80% of samples, this means 20% of samples aren't used for the model to learn anything.\n",
    "\n",
    "This also means depending on what 80% is used to train on and what 20% is used to evaluate the model, it may achieve a score which doesn't reflect the entire dataset.\n",
    "\n",
    "For example, if a lot of easy examples are in the 80% training data, when it comes to test on the 20%, your model may perform poorly.\n",
    "\n",
    "The same goes for the reverse.\n",
    "\n",
    "Figure 2.0 shows 5-fold cross-validation, a method which tries to provide a solution to:\n",
    "\n",
    "1. Not training on all the data (always keeping training and test sets separate).\n",
    "2. Avoiding getting lucky scores on single splits of the data.\n",
    "\n",
    "Instead of training only on 1 training split and evaluating on 1 testing split, 5-fold cross-validation does it 5 times.\n",
    "\n",
    "On a different split each time, returning a score for each.\n",
    "\n",
    "Why 5-fold?\n",
    "\n",
    "The actual name of this setup K-fold cross-validation. Where K is an abitrary number. We've used 5 because it looks nice visually, and it is the default value in [`sklearn.model_selection.cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html).\n",
    "\n",
    "Figure 2.0 is what happens when we run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tb71VhgopTd8",
    "outputId": "b2149abc-9bd4-42eb-82b2-51c949113580"
   },
   "outputs": [],
   "source": [
    "# 5-fold cross-validation\n",
    "cross_val_score(clf, X, y, cv=5) # cv is equivalent to K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNUGtjmtpTd8"
   },
   "source": [
    "Since we set `cv=5` (5-fold cross-validation), we get back 5 different scores instead of 1.\n",
    "\n",
    "Taking the mean of this array gives us a more in-depth idea of how our model is performing by converting the 5 scores into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5W6_vFk9pTd8",
    "outputId": "79eeff72-22bf-452c-cc08-654d5b0dd812"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Single training and test split score\n",
    "clf_single_score = clf.score(X_test, y_test)\n",
    "\n",
    "# Take mean of 5-fold cross-validation\n",
    "clf_cross_val_score = np.mean(cross_val_score(clf, X, y, cv=5))\n",
    "\n",
    "clf_single_score, clf_cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7PeTiBWpTd8"
   },
   "source": [
    "Notice, the average `cross_val_score()` is slightly lower than single value returned by `score()`.\n",
    "\n",
    "In this case, if you were asked to report the accuracy of your model, even though it's lower, you'd prefer the cross-validated metric over the non-cross-validated metric.\n",
    "\n",
    "Wait?\n",
    "\n",
    "We haven't used the `scoring` parameter at all.\n",
    "\n",
    "By default, it's set to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LK_dUsrpTd9",
    "outputId": "ae38c48e-8de2-434f-aa5d-241d7143c0d6"
   },
   "outputs": [],
   "source": [
    "cross_val_score(clf, X, y, cv=5, scoring=None) # default scoring value, this can be set to other scoring metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08YVsVVrpTd9"
   },
   "source": [
    "> **Note:** If you notice different scores each time you call `cross_val_score`, this is because each data split is random every time. So the model may achieve higher/lower scores on different splits of the data. To get reproducible scores, you can set the random seed.\n",
    "\n",
    "When `scoring` is set to `None` (by default), it uses the same metric as `score()` for whatever model is passed to `cross_val_score()`.\n",
    "\n",
    "In this case, our model is `clf` which is an instance of `RandomForestClassifier` which uses mean accuracy as the default `score()` metric.\n",
    "\n",
    "You can change the evaluation score `cross_val_score()` uses by changing the `scoring` parameter.\n",
    "\n",
    "And as you might have guessed, different problems call for different evaluation scores.\n",
    "\n",
    "The [Scikit-Learn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) outlines a vast range of evaluation metrics for different problems but let's have a look at a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeBn3BHupTd9"
   },
   "source": [
    "### 4.2.1 Classification model evaluation metrics\n",
    "\n",
    "Four of the main evaluation metrics/methods you'll come across for classification models are:\n",
    "\n",
    "1. [Accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy)\n",
    "2. [Area under ROC curve](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) (receiver operating characteristic curve)\n",
    "3. [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "4. [Classification report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "\n",
    "Let's have a look at each of these. We'll bring down the classification code from above to go through some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yg9t0jVLpTd9",
    "outputId": "0327f1e0-7c0d-4782-e8ca-f1351f291052"
   },
   "outputs": [],
   "source": [
    "# Import cross_val_score from the model_selection module\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuuTaAWdpTd9"
   },
   "source": [
    "#### Accuracy\n",
    "Accuracy is the default metric for the `score()` function within each of Scikit-Learn's classifier models. And it's probably the metric you'll see most often used for classification problems.\n",
    "\n",
    "However, we'll see in a second how it may not always be the best metric to use.\n",
    "\n",
    "Scikit-Learn returns accuracy as a decimal but you can easily convert it to a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZwG2F2NpTd9",
    "outputId": "936301f7-b1f7-4811-829e-b2f2300b13a8"
   },
   "outputs": [],
   "source": [
    "# Accuracy as percentage\n",
    "print(f\"Heart Disease Classifier Accuracy: {clf.score(X_test, y_test) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTSc3PdWpTd-"
   },
   "source": [
    "#### Area Under Receiver Operating Characteristic (ROC) Curve\n",
    "If this one sounds like a mouthful, its because reading the full name is.\n",
    "\n",
    "It's usually referred to as AUC for Area Under Curve and the curve they're talking about is the Receiver Operating Characteristic or ROC for short.\n",
    "\n",
    "So if hear someone talking about AUC or ROC, they're probably talking about what follows.\n",
    "\n",
    "ROC curves are a comparison of true postive rate (tpr) versus false positive rate (fpr).\n",
    "\n",
    "For clarity:\n",
    "* True positive = model predicts 1 when truth is 1\n",
    "* False positive = model predicts 1 when truth is 0\n",
    "* True negative = model predicts 0 when truth is 0\n",
    "* False negative = model predicts 0 when truth is 1\n",
    "\n",
    "Now we know this, let's see one. Scikit-Learn lets you calculate the information required for a ROC curve using the [`roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OKdNdMBpTd-",
    "outputId": "128c89a4-4d5e-4773-ba5e-ef4c172f5e0e"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Make predictions with probabilities\n",
    "y_probs = clf.predict_proba(X_test)\n",
    "\n",
    "# Keep the probabilites of the positive class only\n",
    "y_probs = y_probs[:, 1]\n",
    "\n",
    "# Calculate fpr, tpr and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "\n",
    "# Check the false positive rate\n",
    "fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKD3drzzpTd-"
   },
   "source": [
    "Looking at these on their own doesn't make much sense. It's much easier to see their value visually.\n",
    "\n",
    "Let's create a helper function to make a ROC curve given the false positive rates (`fpr`) and true positive rates (`tpr`).\n",
    "\n",
    "> **Note:** As of Scikit-Learn 1.2+, there is functionality of plotting a ROC curve. You can find this under [`sklearn.metrics.RocCurveDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html#sklearn-metrics-roccurvedisplay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37QtYcMvpTd-",
    "outputId": "3c50273d-5b68-4a54-c9c3-396f1d9d606f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    \"\"\"\n",
    "    Plots a ROC curve given the false positve rate (fpr) and\n",
    "    true postive rate (tpr) of a classifier.\n",
    "    \"\"\"\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    # Plot line with no predictive power (baseline)\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--', label='Guessing')\n",
    "    # Customize the plot\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIiZBYMnpTd-"
   },
   "source": [
    "Looking at the plot for the first time, it might seem a bit confusing.\n",
    "\n",
    "The main thing to take away here is our model is doing far better than guessing.\n",
    "\n",
    "A metric you can use to quantify the ROC curve in a single number is AUC (Area Under Curve).\n",
    "\n",
    "Scikit-Learn implements a function to caculate this called [`sklearn.metrics.roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score).\n",
    "\n",
    "The maximum ROC AUC score you can achieve is 1.0 and generally, the closer to 1.0, the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXe4MJyrpTd-",
    "outputId": "9ca51027-d962-42b0-9c96-9b1580ac5bdc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score_value = roc_auc_score(y_test, y_probs)\n",
    "roc_auc_score_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFLl8LZWpTd_"
   },
   "source": [
    "I'll let you in a secret...\n",
    "\n",
    "Although it was good practice, we didn't actually need to create our own `plot_roc_curve` function.\n",
    "\n",
    "Scikit-Learn allows us to plot a ROC curve directly from our estimator/model by using the class method [`sklearn.metrics.RocCurveDisplay.from_estimator`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html#sklearn.metrics.RocCurveDisplay.from_estimator) and passing it our `estimator`, `X_test` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOROafyhpTd_",
    "outputId": "5c2922a0-728e-443a-830d-bcc3f1fdb949"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "roc_curve_display = RocCurveDisplay.from_estimator(estimator=clf,\n",
    "                                                   X=X_test,\n",
    "                                                   y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vColjAdtpTd_"
   },
   "source": [
    "The most ideal position for a ROC curve to run along the top left corner of the plot.\n",
    "\n",
    "This would mean the model predicts only true positives and no false positives. And would result in a ROC AUC score of 1.0.\n",
    "\n",
    "You can see this by creating a ROC curve using only the `y_test` labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ft40m515pTd_",
    "outputId": "639928a9-08cc-42a9-a190-629586ebd152"
   },
   "outputs": [],
   "source": [
    "# Plot perfect ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test)\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92QSB6_FpTd_",
    "outputId": "0b96db0e-c048-42c5-efdb-a1fca691caed"
   },
   "outputs": [],
   "source": [
    "# Perfect ROC AUC score\n",
    "roc_auc_score(y_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaM5DoXHpTd_"
   },
   "source": [
    "In reality, a perfect ROC curve is unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uK4UGr58pTeA"
   },
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "Another fantastic way to evaluate a classification model is by using a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "\n",
    "A confusion matrix is a quick way to compare the labels a model predicts and the actual labels it was supposed to predict.\n",
    "\n",
    "In essence, giving you an idea of where the model is getting confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_S60A5PpTeA",
    "outputId": "8d642200-0722-4059-e157-55883a261ca3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKwUszr1pTeA"
   },
   "source": [
    "Again, this is probably easier visualized.\n",
    "\n",
    "One way to do it is with `pd.crosstab()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CO__kdHMpTeA",
    "outputId": "ff604b1f-e7f3-47b0-f04a-a36c75d4704f"
   },
   "outputs": [],
   "source": [
    "pd.crosstab(y_test,\n",
    "            y_preds,\n",
    "            rownames=[\"Actual Label\"],\n",
    "            colnames=[\"Predicted Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB_F2w1hpTeA"
   },
   "source": [
    "#### Creating a confusion matrix using Scikit-Learn\n",
    "\n",
    "Scikit-Learn has multiple different implementations of plotting confusion matrices:\n",
    "\n",
    "1. [`sklearn.metrics.ConfusionMatrixDisplay.from_estimator(estimator, X, y)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator) - this takes a fitted estimator (like our `clf` model), features (`X`) and labels (`y`), it then uses the trained estimator to make predictions on `X` and compares the predictions to `y` by displaying a confusion matrix.\n",
    "2. [`sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_true, y_pred)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions) - this takes truth labels and predicted labels and compares them by displaying a confusion matrix.\n",
    "\n",
    "> **Note:** Both of these methods/classes require Scikit-Learn 1.0+. To check your version of Scikit-Learn run:\n",
    "```python\n",
    "import sklearn\n",
    "sklearn.__version__\n",
    "```\n",
    "> If you don't have 1.0+, you can upgrade at: https://scikit-learn.org/stable/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEehieUwpTeA",
    "outputId": "53c57caf-359d-4a0c-be06-d6760c3657d1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=clf, X=X, y=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWbAXxJlpTeB",
    "outputId": "1bc060da-3ed6-4cdc-81fe-c6bd155a14cc"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix from predictions\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=y_test,\n",
    "                                        y_pred=y_preds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKYmb1YlpTeB"
   },
   "source": [
    "#### Classification report\n",
    "\n",
    "The final major metric you should consider when evaluating a classification model is a classification report.\n",
    "\n",
    "A classification report is more so a collection of metrics rather than a single one.\n",
    "\n",
    "You can create a classification report using Scikit-Learn's [`sklearn.metrics.classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) method.\n",
    "\n",
    "Let's see one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXFynPCgpTeB",
    "outputId": "d7e82e56-9db3-442a-e5e1-3a4ffd36a76a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAfh_PwxpTeB"
   },
   "source": [
    "It returns four columns: precision, recall, f1-score and support.\n",
    "\n",
    "The number of rows will depend on how many different classes there are. But there will always be three rows labell accuracy, macro avg and weighted avg.\n",
    "\n",
    "Each term measures something slightly different:\n",
    "* **Precision** - Indicates the proportion of positive identifications (model predicted class `1`) which were actually correct. A model which produces no false positives has a precision of 1.0.\n",
    "* **Recall** - Indicates the proportion of actual positives which were correctly classified. A model which produces no false negatives has a recall of 1.0.\n",
    "* **F1 score** - A combination of precision and recall. A perfect model achieves an F1 score of 1.0.\n",
    "* **Support** - The number of samples each metric was calculated on.\n",
    "* **Accuracy** - The accuracy of the model in decimal form. Perfect accuracy is equal to 1.0, in other words, getting the prediction right 100% of the time.\n",
    "* **Macro avg** - Short for macro average, the average precision, recall and F1 score between classes. Macro avg doesn't take class imbalance into effect. So if you do have class imbalances (more examples of one class than another), you should pay attention to this.\n",
    "* **Weighted avg** - Short for weighted average, the weighted average precision, recall and F1 score between classes. Weighted means each metric is calculated with respect to how many samples there are in each class. This metric will favour the majority class (e.g. it will give a high value when one class out performs another due to having more samples).\n",
    "\n",
    "When should you use each?\n",
    "\n",
    "It can be tempting to base your classification models perfomance only on accuracy. And accuracy is a good metric to report, except when you have very imbalanced classes.\n",
    "\n",
    "For example, let's say there were 10,000 people. And 1 of them had a disease. You're asked to build a model to predict who has it.\n",
    "\n",
    "You build the model and find your model to be 99.99% accurate. Which sounds great!\n",
    "...until you realise, all its doing is predicting no one has the disease, in other words all 10,000 predictions are false.\n",
    "\n",
    "In this case, you'd want to turn to metrics such as precision, recall and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roYbY6CkpTeB",
    "outputId": "c0fc3cc9-a241-4789-8803-727388031aaa"
   },
   "outputs": [],
   "source": [
    "# Where precision and recall become valuable\n",
    "disease_true = np.zeros(10000)\n",
    "disease_true[0] = 1 # only one case\n",
    "\n",
    "disease_preds = np.zeros(10000) # every prediction is 0\n",
    "\n",
    "pd.DataFrame(classification_report(disease_true,\n",
    "                                   disease_preds,\n",
    "                                   output_dict=True,\n",
    "                                   zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ixccim-9pTeC"
   },
   "source": [
    "You can see here, we've got an accuracy of 0.9999 (99.99%), great precision and recall on class 0.0 but nothing for class 1.0.\n",
    "\n",
    "Ask yourself, although the model achieves 99.99% accuracy, is it useful?\n",
    "\n",
    "To summarize:\n",
    "* Accuracy is a good measure to start with if all classes are balanced (e.g. same amount of samples which are labelled with 0 or 1)\n",
    "* Precision and recall become more important when classes are imbalanced.\n",
    "* If false positive predictions are worse than false negatives, aim for higher precision.\n",
    "* If false negative predictions are worse than false positives, aim for higher recall.\n",
    "\n",
    "> **Resource:** For more on precision and recall and the tradeoffs between them, I'd suggest going through the [Scikit-Learn Precision-Recall guide](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4qjN-9-pTeC"
   },
   "source": [
    "### 4.2.2 Regression model evaluation metrics\n",
    "\n",
    "Similar to classification, there are [several metrics you can use to evaluate your regression models](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics).\n",
    "\n",
    "We'll check out the following.\n",
    "\n",
    "1. **R^2 (pronounced r-squared) or coefficient of determination** - Compares your models predictions to the mean of the targets. Values can range from negative infinity (a very poor model) to 1. For example, if all your model does is predict the mean of the targets, its R^2 value would be 0. And if your model perfectly predicts a range of numbers it's R^2 value would be 1. Higher is better.\n",
    "2. **Mean absolute error (MAE)** - The average of the absolute differences between predictions and actual values. It gives you an idea of how wrong your predictions were. Lower is better.\n",
    "3. **Mean squared error (MSE)** - The average squared differences between predictions and actual values. Squaring the errors removes negative errors. It also amplifies outliers (samples which have larger errors). Lower is better.\n",
    "\n",
    "Let's see them in action. First, we'll bring down our regression model code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAodhcXnpTeC"
   },
   "outputs": [],
   "source": [
    "# Import the RandomForestRegressor model class from the ensemble module\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Institate and fit the model (on the training set)\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "588EAhDLpTeC"
   },
   "source": [
    "**R^2 Score (coefficient of determination)**\n",
    "\n",
    "Once you've got a trained regression model, the default evaluation metric in the `score()` function is R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mygqXjucpTeD",
    "outputId": "0685b94a-918d-4d4a-a0c7-31f938b2e9ce"
   },
   "outputs": [],
   "source": [
    "# Calculate the models R^2 score\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTSwaFJdpTeD"
   },
   "source": [
    "Outside of the `score()` function, R^2 can be calculated using Scikit-Learn's [`r2_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) function.\n",
    "\n",
    "A model which only predicted the mean would get a score of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gU1wmv3pTeD",
    "outputId": "1518c649-969b-4f7f-8f2b-8de0ebfa2650"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Fill an array with y_test mean\n",
    "y_test_mean = np.full(len(y_test), y_test.mean())\n",
    "\n",
    "r2_score(y_test, y_test_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0aPFJakpTeD"
   },
   "source": [
    "And a perfect model would get a score of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlnM0P0kpTeE",
    "outputId": "f55b622e-8ef5-4e32-8932-31f4dddcd046"
   },
   "outputs": [],
   "source": [
    "r2_score(y_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "at0isNC9pTeE"
   },
   "source": [
    "For your regression models, you'll want to maximise R^2, whilst minimising MAE and MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrHjsaUmpTeE"
   },
   "source": [
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "A model's mean absolute error can be calculated with Scikit-Learn's [`sklearn.metrics.mean_absolute_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIfMfTPMpTeE",
    "outputId": "13b81067-b964-41c1-ff2e-a10f745bdcb4"
   },
   "outputs": [],
   "source": [
    "# Mean absolute error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_preds = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_preds)\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvFFWXtepTeE"
   },
   "source": [
    "Our model achieves an MAE of 0.327.\n",
    "\n",
    "This means, on average our models predictions are 0.327 units away from the actual value.\n",
    "\n",
    "Let's make it a little more visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31SrvpnNpTeF",
    "outputId": "0cafd3f1-c642-406e-b234-db8491e399a8"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"actual values\": y_test,\n",
    "                   \"predictions\": y_preds})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31I-ta-mpTeF"
   },
   "source": [
    "You can see the predictions are slightly different to the actual values.\n",
    "\n",
    "Depending what problem you're working on, having a difference like we do now, might be okay. On the flip side, it may also not be okay, meaning the predictions would have to be closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQCMtM5upTeF",
    "outputId": "9ebcb411-e8c2-40cf-96ba-61e57535e9d9"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.arange(0, len(df), 1)\n",
    "ax.scatter(x, df[\"actual values\"], c='b', label=\"Acutual Values\")\n",
    "ax.scatter(x, df[\"predictions\"], c='r', label=\"Predictions\")\n",
    "ax.legend(loc=(1, 0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GzMwDTDpTeF"
   },
   "source": [
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "How about MSE?\n",
    "\n",
    "We can calculate it with Scikit-Learn's [`sklearn.metrics.mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNjphWg4pTeF",
    "outputId": "9e3565b0-fd25-4fee-ca0d-370618b800da"
   },
   "outputs": [],
   "source": [
    "# Mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, y_preds)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVZLE7hZpTeF"
   },
   "source": [
    "MSE will often be higher than MAE because is squares the errors rather than only taking the absolute difference into account.\n",
    "\n",
    "Now you might be thinking, which regression evaluation metric should you use?\n",
    "\n",
    "* R^2 is similar to accuracy. It gives you a quick indication of how well your model might be doing. Generally, the closer your R^2 value is to 1.0, the better the model. But it doesn't really tell exactly how wrong your model is in terms of how far off each prediction is.\n",
    "* MAE gives a better indication of how far off each of your model's predictions are on average.\n",
    "* As for MAE or MSE, because of the way MSE is calculated, squaring the differences between predicted values and actual values, it amplifies larger differences. Let's say we're predicting the value of houses (which we are).\n",
    "    * Pay more attention to MAE: When being \\$10,000 off is ***twice*** as bad as being \\$5,000 off.\n",
    "    * Pay more attention to MSE: When being \\$10,000 off is ***more than twice*** as bad as being \\$5,000 off.\n",
    "    \n",
    "> **Note:** What we've covered here is only a handful of potential metrics you can use to evaluate your models. If you're after a complete list, check out the [Scikit-Learn metrics and scoring documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbZkdj-lpTeG"
   },
   "source": [
    "### 4.2.3 Evaluating a model using the `scoring` parameter\n",
    "\n",
    "We've covered a bunch of ways to evaluate a model's predictions but haven't even touched the `scoring` parameter...\n",
    "\n",
    "Not to worry, it's very similar to what we've been doing!\n",
    "\n",
    "As a refresh, the `scoring` parameter can be used with a function like `cross_val_score()` to tell Scikit-Learn what evaluation metric to return using cross-validation.\n",
    "\n",
    "Let's check it out with our classification model and the heart disease dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tO2Ycq-XpTeG"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9vplJqIpTeG"
   },
   "source": [
    "First, we'll use the default, which is mean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4h67M10apTeG",
    "outputId": "5c825d48-b98a-4fe3-cde9-94e81f374010"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_acc = cross_val_score(clf, X, y, cv=5)\n",
    "cv_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kmf0SJHupTeG"
   },
   "source": [
    "We've seen this before, now we got 5 different accuracy scores on different test splits of the data.\n",
    "\n",
    "Averaging this gives the cross-validated accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sBgx6g1pTeG",
    "outputId": "99941f14-b51f-42b3-f061-54064f09bdf5"
   },
   "outputs": [],
   "source": [
    "# Cross-validated accuracy\n",
    "print(f\"The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_1GjT3ApTeH"
   },
   "source": [
    "We can find the same using the `scoring` parameter and passing it `\"accuracy\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l77ZnEEfpTeH",
    "outputId": "51600484-ab56-40b6-fcda-2e372a083d6f"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_acc = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")\n",
    "print(f\"The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3tuXUT9pTeH"
   },
   "source": [
    "The same goes for the other metrics we've been using for classification.\n",
    "\n",
    "Let's try `\"precision\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FznelboSpTeH",
    "outputId": "40909f65-39ff-46a7-f3a9-b1877d1808a6"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_precision = cross_val_score(clf, X, y, cv=5, scoring=\"precision\")\n",
    "print(f\"The cross-validated precision is: {np.mean(cv_precision):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpboHJptpTeH"
   },
   "source": [
    "How about `\"recall\"`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3a3_xC8pTeH",
    "outputId": "2d843035-8976-4655-ef0d-965965ce2e84"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_recall = cross_val_score(clf, X, y, cv=5, scoring=\"recall\")\n",
    "print(f\"The cross-validated recall is: {np.mean(cv_recall):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9H06ys64pTeH"
   },
   "source": [
    "And `\"f1\"` (for F1 score)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHj3L6BPpTeI",
    "outputId": "30a7aa23-d88a-4fb9-d2ca-de767ca943ba"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_f1 = cross_val_score(clf, X, y, cv=5, scoring=\"f1\")\n",
    "print(f\"The cross-validated F1 score is: {np.mean(cv_f1):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264Hezr0pTeI"
   },
   "source": [
    "We can repeat this process with our regression metrics.\n",
    "\n",
    "Let's revisit our regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMLzP-1gpTeI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmbIBVHJpTeI"
   },
   "source": [
    "The default is `\"r2\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gw2U9DcApTeI",
    "outputId": "5c0627f2-5bbd-48ac-c38f-c98f33c8af97"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_r2 = cross_val_score(model, X, y, cv=5, scoring=\"r2\")\n",
    "print(f\"The cross-validated R^2 score is: {np.mean(cv_r2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14_JOMDypTeI"
   },
   "source": [
    "But we can use `\"neg_mean_absolute_error\"` for MAE (mean absolute error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByTPCa2KpTeI",
    "outputId": "9b47ec07-2d23-4518-daa5-05220cf985e3"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_mae = cross_val_score(model, X, y, cv=5, scoring=\"neg_mean_absolute_error\")\n",
    "print(f\"The cross-validated MAE score is: {np.mean(cv_mae):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4LYgONzpTeI"
   },
   "source": [
    "Why the `\"neg_\"`?\n",
    "\n",
    "Because Scikit-Learn documentation states:\n",
    "> [\"All scorer objects follow the convention that higher return values are better than lower return values.\"](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)\n",
    "\n",
    "Which in this case, means a lower negative value (closer to 0) is better.\n",
    "\n",
    "What about `\"neg_mean_squared_error\"` for MSE (mean squared error)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHMtwODjpTeI",
    "outputId": "ae67aa2b-6657-4aac-f59c-1950112db146"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_mse = cross_val_score(model,X,y,cv=5,scoring=\"neg_mean_squared_error\")\n",
    "print(f\"The cross-validated MSE score is: {np.mean(cv_mse):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozvl96UypTeJ"
   },
   "source": [
    "### 4.3 Using different evaluation metrics with Scikit-Learn\n",
    "\n",
    "Remember the third way of evaluating Scikit-Learn functions?\n",
    "\n",
    "> 3. Problem-specific metric functions. Similar to how the `scoring` parameter can be passed different scoring functions, Scikit-Learn implements these as stand alone functions.\n",
    "\n",
    "Well, we've kind of covered this third way of using evaulation metrics with Scikit-Learn.\n",
    "\n",
    "In essence, all of the metrics we've seen previously have their own function in Scikit-Learn.\n",
    "\n",
    "They all work by comparing an array of predictions, usually called `y_preds` to an array of actual labels, usually called `y_test` or `y_true`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jPltmL7pTeJ"
   },
   "source": [
    "#### Classification functions\n",
    "For:\n",
    "* Accuracy we can use [`sklearn.metrics.accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "* Precision we can use [`sklearn.metrics.precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "* Recall we can use [`sklearn.metrics.recall_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "* F1 we can use [`sklearn.metrics.f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNd2SXj9pTeJ",
    "outputId": "c3cc742f-2fec-41f6-dc49-4deaef8a5e44"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Classifier metrics on the test set:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_preds) * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(y_test, y_preds):.2f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_preds):.2f}\")\n",
    "print(f\"F1: {f1_score(y_test, y_preds):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NphQu54GpTeJ"
   },
   "source": [
    "#### Regression metrics\n",
    "\n",
    "We can use a similar setup for our regression problem, just with different methods.\n",
    "\n",
    "For:\n",
    "* R^2 we can use [`sklearn.metrics.r2_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)\n",
    "* MAE (mean absolute error) we can use [`sklearn.metrics.mean_absolute_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html)\n",
    "* MSE (mean squared error) we can use [`sklearn.metrics.mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHGPft6lpTeJ",
    "outputId": "63b28288-fbe6-459f-9928-36611af48d4c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100,\n",
    "                              n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Regression model metrics on the test set:\")\n",
    "print(f\"R^2: {r2_score(y_test, y_preds):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_preds):.2f}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_preds):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1_fbTEfpTeJ"
   },
   "source": [
    "Wow!\n",
    "\n",
    "We've covered a lot!\n",
    "\n",
    "But it's worth it.\n",
    "\n",
    "Because evaluating a model's predictions is as important as training a model in any machine learning project.\n",
    "\n",
    "There's nothing worse than training a machine learning model and optimizing for the wrong evaluation metric.\n",
    "\n",
    "Keep the metrics and evaluation methods we've gone through when training your future models.\n",
    "\n",
    "If you're after extra reading, I'd go through the [Scikit-Learn guide for model evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "\n",
    "Now we've seen some different metrics we can use to evaluate a model, let's see some ways we can improve those metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIPRO0XEpTeK"
   },
   "source": [
    "## 5. Improving model predictions through experimentation (hyperparameter tuning)\n",
    "\n",
    "The first predictions you make with a model are generally referred to as **baseline predictions**.\n",
    "\n",
    "It's similar for the first evaluation metrics you get. These are generally referred to as **baseline metrics**.\n",
    "\n",
    "Your next goal is to improve upon these baseline metrics.\n",
    "\n",
    "How?\n",
    "\n",
    "*Experiment, experiment, experiment!*\n",
    "\n",
    "Two of the main methods to improve baseline metrics are:\n",
    "1. From a data perspective.\n",
    "2. From a model perspective.\n",
    "\n",
    "From a data perspective asks:\n",
    "* Could we collect more data? In machine learning, more data is generally better, as it gives a model more opportunities to learn patterns.\n",
    "* Could we improve our data? This could mean filling in misisng values or finding a better encoding (turning data into numbers) strategy.\n",
    "\n",
    "From a model perspective asks:\n",
    "* Is there a better model we could use? If you've started out with a simple model, could you use a more complex one? (we saw an example of this when looking at the [Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html), ensemble methods are generally considered more complex models)\n",
    "* Could we improve the current model? If the model you're using performs well straight out of the box, can the hyperparameters be tuned to make it even better?\n",
    "\n",
    "> **Note:** Patterns in data are also often referred to as data parameters. The difference between *parameters* and *hyperparameters* is a machine learning model seeks to find parameters in data on its own, where as, hyperparameters are settings on a model which a person (you) can adjust.\n",
    "\n",
    "Since we have two existing datasets, we'll look at improving our results from a model perspective.\n",
    "\n",
    "More specifically, we'll look at how we could improve our `RandomForestClassifier` and `RandomForestRegressor` models through hyperparameter tuning.\n",
    "\n",
    "What even are hyperparameters?\n",
    "\n",
    "Good question, let's check them out.\n",
    "\n",
    "First, we'll instantiate a `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0u61xAFpTeK"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuFqtRVNpTeK"
   },
   "source": [
    "When we instantiate a model like above, we're using the default hyperparameters.\n",
    "\n",
    "These get printed out when you call the model instance and `get_params()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kfCJyBBpTeK",
    "outputId": "22fcaab6-c177-487a-a522-33d2ddb655cb"
   },
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5BrB2lSpTeK"
   },
   "source": [
    "You'll see things like `max_depth`, `min_samples_split`, `n_estimators`.\n",
    "\n",
    "Each of these is a hyperparameter of the `RandomForestClassifier` you can adjust.\n",
    "\n",
    "You can think of hyperparameters as being similar to dials on an oven.\n",
    "\n",
    "On the default setting your oven might do an okay job cooking your favourite meal. But with a little experimentation, you find it does better when you adjust the settings.\n",
    "\n",
    "<img src=\"https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/images/sklearn-hyperparameter-tuning-oven.png?raw=1\" width=500/>\n",
    "\n",
    "The same goes for imporving a machine learning model by hyperparameter tuning.\n",
    "\n",
    "The default hyperparameters on a machine learning model may find patterns in data well. But there's a chance a adjusting the hyperparameters may improve a models performance.\n",
    "\n",
    "Every machine learning model will have different hyperparameters you can tune.\n",
    "\n",
    "You might be thinking, \"how the hell do I remember all of these?\"\n",
    "\n",
    "Another good question.\n",
    "\n",
    "It's why we're focused on the Random Forest.\n",
    "\n",
    "Instead of memorizing all of the hyperparameters for every model, we'll see how it's done with one.\n",
    "\n",
    "And then knowing these principles, you can apply them to a different model if needed.\n",
    "\n",
    "Reading the [Scikit-Learn documentation for the Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), you'll find they suggest trying to change `n_estimators` (the number of trees in the forest) and `min_samples_split` (the minimum number of samples required to split an internal node).\n",
    "\n",
    "We'll try tuning these as well as:\n",
    "* `max_features` (the number of features to consider when looking for the best split)\n",
    "* `max_depth` (the maximum depth of the tree)\n",
    "* `min_samples_leaf` (the minimum number of samples required to be at a leaf node)\n",
    "\n",
    "If this still sounds like a lot, the good news is, the process we're taking with the Random Forest and tuning its hyperparameters, can be used for other machine learning models in Scikit-Learn. The only difference is, with a different model, the hyperparameters you tune will be different.\n",
    "\n",
    "Adjusting hyperparameters is usually an experimental process to figure out which are best. As there's no real way of knowing which hyperparameters will be best when starting out.\n",
    "\n",
    "To get familar with hyparameter tuning, we'll take our RandomForestClassifier and adjust its hyperparameters in 3 ways.\n",
    "\n",
    "1. By hand\n",
    "2. Randomly with [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "3. Exhaustively with [`sklearn.model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVUZEit9pTeK"
   },
   "source": [
    "### 5.1 Tuning hyperparameters by hand\n",
    "\n",
    "So far we've worked with training and test datasets.\n",
    "\n",
    "You train a model on a training set and evaluate it on a test dataset.\n",
    "\n",
    "But hyperparameter tuning introduces a thrid set, a validation set.\n",
    "\n",
    "Now the process becomes:\n",
    "1. Train a model on the training data.\n",
    "2. (Try to) improve the model's hyperparameters on the validation set.\n",
    "3. Evaluate the model on the test set.\n",
    "\n",
    "If our starting dataset contained 100 different patient records labels indicating who had heart disease and who didn't and we wanted to build a machine learning model to predict who had heart disease and who didn't, it might look like this:\n",
    "\n",
    "<img src=\"https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/images/sklearn-train-valid-test-annotated.png?raw=1\" width=500/>\n",
    "\n",
    "Since we know we're using a `RandomForestClassifier` and we know the hyperparameters we want to adjust, let's see what it looks like.\n",
    "\n",
    "First, let's remind ourselves of the base parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTRxzmblpTeK",
    "outputId": "7b3d24c5-e3f1-45b4-cce3-0f2752da6ae4"
   },
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-5S7U0VpTeL"
   },
   "source": [
    "And we're going to adjust:\n",
    "* `max_depth`\n",
    "* `max_features`\n",
    "* `min_samples_leaf`\n",
    "* `min_samples_split`\n",
    "* `n_estimators`\n",
    "\n",
    "We'll use the same code as before, except this time we'll create a training, validation and test split.\n",
    "\n",
    "With the training set containing 70% of the data and the validation and test sets each containing 15%.\n",
    "\n",
    "Let's get some baseline results, then we'll tune the model.\n",
    "\n",
    "And since we're going to be evaluating a few models, let's make an evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYTP3Hj1pTeL"
   },
   "outputs": [],
   "source": [
    "def evaluate_preds(y_true: np.array,\n",
    "                   y_preds: np.array) -> dict:\n",
    "    \"\"\"\n",
    "    Performs evaluation comparison on y_true labels vs. y_pred labels.\n",
    "\n",
    "    Returns several metrics in the form of a dictionary.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    precision = precision_score(y_true, y_preds)\n",
    "    recall = recall_score(y_true, y_preds)\n",
    "    f1 = f1_score(y_true, y_preds)\n",
    "    metric_dict = {\"accuracy\": round(accuracy, 2),\n",
    "                   \"precision\": round(precision, 2),\n",
    "                   \"recall\": round(recall, 2),\n",
    "                   \"f1\": round(f1, 2)}\n",
    "    print(f\"Acc: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 score: {f1:.2f}\")\n",
    "\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13twJLQCpTeL"
   },
   "source": [
    "Wonderful!\n",
    "\n",
    "Now let's recreate a previous workflow, except we'll add in the creation of a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSjPRWFzpTeL",
    "outputId": "914b81a9-fbbd-4309-925a-f0a10fbc51c9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set the seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Read in the data\n",
    "heart_disease = pd.read_csv(\"data/heart-disease.csv\")\n",
    "\n",
    "# Split into X (features) & y (labels)\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Training and test split (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Create validation and test split by spliting testing data in half (30% test -> 15% validation, 15% test)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_preds = clf.predict(X_valid)\n",
    "\n",
    "# Evaluate the classifier\n",
    "baseline_metrics = evaluate_preds(y_valid, y_preds)\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRKvO952pTeL",
    "outputId": "1dfa63cb-1599-49cf-abee-2b785630f251"
   },
   "outputs": [],
   "source": [
    "# Check the sizes of the splits\n",
    "print(f\"Training data: {len(X_train)} samples, {len(y_train)} labels\")\n",
    "print(f\"Validation data: {len(X_valid)} samples, {len(y_valid)} labels\")\n",
    "print(f\"Testing data: {len(X_test)} samples, {len(y_test)} labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7NJ9tBdpTeL"
   },
   "source": [
    "Beautiful, now let's try and improve the results.\n",
    "\n",
    "We'll change 1 of the hyperparameters, `n_estimators=100` (default) to `n_estimators=200` and see if it improves on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfmMKUtmpTeL",
    "outputId": "8cf6aa94-8812-401b-b7b1-c853ceccbb18"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Create a second classifier\n",
    "clf_2 = RandomForestClassifier(n_estimators=200)\n",
    "clf_2.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_preds_2 = clf_2.predict(X_valid)\n",
    "\n",
    "# Evaluate the 2nd classifier\n",
    "clf_2_metrics = evaluate_preds(y_valid, y_preds_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EU8A6jUpTeL"
   },
   "source": [
    "Hmm, it looks like doubling the `n_estimators` value performs *worse* than the default, perhaps there's a better value for `n_estimators`?\n",
    "\n",
    "And what other hyperparameters could we change?\n",
    "\n",
    "Wait...\n",
    "\n",
    "This could take a while if all we're doing is building new models with new hyperparameters each time.\n",
    "\n",
    "Surely there's a better way?\n",
    "\n",
    "There is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ct_WwDLdpTeL"
   },
   "source": [
    "### 5.2 Hyperparameter tuning with [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "Scikit-Learn's [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) allows us to randomly search across different hyperparameters to see which work best.\n",
    "\n",
    "It also stores details about the ones which work best!\n",
    "\n",
    "Let's see it in action.\n",
    "\n",
    "First, we create a dictionary of parameter distributions (collections of different values for specific hyperparamters) we'd like to search over.\n",
    "\n",
    "This dictionary comes in the form:\n",
    "\n",
    "```python\n",
    "param_distributions = {\"hyperparameter_name\": [values_to_randomly_try]}\n",
    "```\n",
    "\n",
    "Where `\"hyperparameter_name\"` is the value of a specific hyperparameter for a model and `[values_to_randomly_try]` is a list of values for that specific hyperparamter to randomly try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MD1wInspTeM"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter grid RandomizedSearchCV will search over\n",
    "param_distributions = {\"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n",
    "                       \"max_depth\": [None, 5, 10, 20, 30],\n",
    "                       \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "                       \"min_samples_split\": [2, 4, 6, 8],\n",
    "                       \"min_samples_leaf\": [1, 2, 4, 8]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6g7ysRppTeM"
   },
   "source": [
    "Where did these values come from?\n",
    "\n",
    "They're made up.\n",
    "\n",
    "Made up?\n",
    "\n",
    "Yes.\n",
    "\n",
    "Not completely pulled out of the air but after reading the [Scikit-Learn documentation on Random Forest's](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) you'll see some of these values have certain values which usually perform well and certain hyperparameters take strings rather than integers.\n",
    "\n",
    "Now we've got the parameter distribution dictionary setup, Scikit-Learn's `RandomizedSearchCV` will look at it, pick a random value from each, instantiate a model with those values and test each model.\n",
    "\n",
    "How many models will it test?\n",
    "\n",
    "As many as there are for each combination of hyperparameters to be tested. Let's add them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaVRFwLWpTeM",
    "outputId": "de98ed80-a9ab-4e31-81e1-be05724daf8a"
   },
   "outputs": [],
   "source": [
    "# Count the total number of hyperparameter combinations to test\n",
    "total_randomized_hyperparameter_combintions_to_test = np.prod([len(value) for value in param_distributions.values()])\n",
    "print(f\"There are {total_randomized_hyperparameter_combintions_to_test} potential combinations of hyperparameters to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQKsAUlipTeM"
   },
   "source": [
    "Woah!\n",
    "\n",
    "That's a lot of combinations!\n",
    "\n",
    "Or...\n",
    "\n",
    "We can set the `n_iter` parameter to limit the number of models `RandomizedSearchCV` tests (e.g. `n_iter=20` means to try `20` different random combintations of hyperparameters and will cross-validate each set, so if `cv=5`, 5x20 = 100 total fits).\n",
    "\n",
    "The best thing?\n",
    "\n",
    "The results we get will be cross-validated (hence the CV in `RandomizedSearchCV`) so we can use `train_test_split()`.\n",
    "\n",
    "And since we're going over so many different models, we'll set `n_jobs=-1` in our [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) so Scikit-Learn takes advantage of all the cores (processors) on our computers.\n",
    "\n",
    "Let's see it in action.\n",
    "\n",
    "> **Note:** Depending on `n_iter` (how many models you test), the different values in the hyperparameter grid, and the power of your computer, running the cell below may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diSCrTUSpTeM",
    "outputId": "f1b0664c-c1c7-4ed6-f15d-ffc520b64589"
   },
   "outputs": [],
   "source": [
    "# Start the timer\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into X & y\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Set n_jobs to -1 to use all available cores on your machine (if this causes errors, try n_jobs=1)\n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "n_iter = 30 # try 30 models total\n",
    "rs_clf = RandomizedSearchCV(estimator=clf,\n",
    "                            param_distributions=param_distributions,\n",
    "                            n_iter=n_iter,\n",
    "                            cv=5, # 5-fold cross-validation\n",
    "                            verbose=2) # print out results\n",
    "\n",
    "# Fit the RandomizedSearchCV version of clf (does cross-validation for us, so no need to use a validation set)\n",
    "rs_clf.fit(X_train, y_train);\n",
    "\n",
    "# Finish the timer\n",
    "end_time = time.time()\n",
    "print(f\"[INFO] Total time taken for {n_iter} random combinations of hyperparameters: {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9ivCd7FpTeM"
   },
   "source": [
    "When `RandomizedSearchCV` goes through `n_iter` combinations of of hyperparameter search space, it stores the best ones in the attribute `best_params_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZKJPeGZpTeN",
    "outputId": "d481325d-0778-43fa-9a7c-fd9b91fd586b"
   },
   "outputs": [],
   "source": [
    "# Find the best hyperparameters found by RandomizedSearchCV\n",
    "rs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaH_7LnspTeN"
   },
   "source": [
    "Now when we call `predict()` on `rs_clf` (our `RandomizedSearchCV` version of our classifier), it'll use the best hyperparameters it found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0z-JJ-77pTeN",
    "outputId": "1a7af814-0b7b-47f7-a77c-ac85e47f61c0"
   },
   "outputs": [],
   "source": [
    "# Make predictions with the best hyperparameters\n",
    "rs_y_preds = rs_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "rs_metrics = evaluate_preds(y_test, rs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4jQSAccpTeN"
   },
   "source": [
    "Excellent!\n",
    "\n",
    "Thanks to `RandomizedSearchCV` testing out a bunch of different hyperparameters, we get a nice boost to all of the evaluation metrics for our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p51zIfS0pTeN"
   },
   "source": [
    "### 5.3 Hyperparameter tuning with [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "There's one more way we could try to improve our model's hyperparamters.\n",
    "\n",
    "And it's with [`sklearn.model_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "The main difference between `GridSearchCV` and `RandomizedSearchCV` is `GridSearchCV` searches across a grid of hyperparamters exhaustively (it will try every combination possible), where as, `RandomizedSearchCV` searches across a grid of hyperparameters randomly (stopping after `n_iter` combinations).\n",
    "\n",
    "`GridSearchCV` also refers to a dictionary of parameter distributions as a parameter grid (via the parameter `param_grid`).\n",
    "\n",
    "For example, let's see our dictionary of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GA0kYwrpTeN",
    "outputId": "b2055df7-d1f1-4ec4-bfcc-5d3137f75768"
   },
   "outputs": [],
   "source": [
    "param_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtPCwl66pTeN"
   },
   "source": [
    "`RandomizedSearchCV` tries `n_iter` combinations of different values.\n",
    "\n",
    "Where as, `GridSearchCV` will try every single possible combination.\n",
    "\n",
    "And if you remember from before when we did the calculation: `max_depth` has 4 values, `max_features` has 2, `min_samples_leaf` has 3, `min_samples_split` has 3, `n_estimators` has 5.\n",
    "\n",
    "That's 4x2x3x3x5 = 360 models!\n",
    "\n",
    "This could take a long time depending on the power of the computer you're using, the amount of data you have and the complexity of the hyperparamters (usually higher values means a more complex model).\n",
    "\n",
    "In our case, the data we're using is relatively small (only ~300 samples).\n",
    "\n",
    "Since we've already tried to find some ideal hyperparameters using `RandomizedSearchCV`, we'll create another hyperparameter grid based on the `best_params_` of `rs_clf` with less options and then try to use `GridSearchCV` to find a more ideal set.\n",
    "\n",
    "In essence, the workflow could be:\n",
    "1. Tune hyperparameters by hand to get a feel of the data/model.\n",
    "2. Create a large set of hyperparameter distributions and search across them randomly with `RandomizedSearchCV`.\n",
    "3. Find the best hyperparameters from 2 and reduce the search space before searching across a smaller subset exhaustively with `GridSearchCV`.\n",
    "\n",
    "> **Note:** Based on the `best_params_` of `rs_clf` implies the next set of hyperparameters we'll try are roughly in the same range of the best set found by `RandomizedSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llQpptuypTeN"
   },
   "outputs": [],
   "source": [
    "# Create hyperparameter grid similar to rs_clf.best_params_\n",
    "param_grid = {\"n_estimators\": [200, 1000],\n",
    "              \"max_depth\": [30, 40, 50],\n",
    "              \"max_features\": [\"log2\"],\n",
    "              \"min_samples_split\": [2, 4, 6, 8],\n",
    "              \"min_samples_leaf\": [4]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REv0o7dWpTeN"
   },
   "source": [
    "We've created another grid of hyperparameters to search over, this time with less total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRsbCyhypTeO",
    "outputId": "c4ab0e2d-13af-4eb1-fba5-8135609dc360"
   },
   "outputs": [],
   "source": [
    "# Count the total number of hyperparameter combinations to test\n",
    "total_grid_search_hyperparameter_combinations_to_test = np.prod([len(value) for value in param_grid.values()])\n",
    "print(f\"There are {total_grid_search_hyperparameter_combinations_to_test} combinations of hyperparameters to test.\")\n",
    "print(f\"This is {total_randomized_hyperparameter_combintions_to_test/total_grid_search_hyperparameter_combinations_to_test} times less\\\n",
    " than before (previous: {total_randomized_hyperparameter_combintions_to_test}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vz9H-031pTeO"
   },
   "source": [
    "Now when we run `GridSearchCV`, passing it our classifier (`clf`), parameter grid (`param_grid`) and the number of cross-validation folds we'd like to use (`cv=5`), it'll create a model with every single combination of hyperparameters, and then cross-validate each 5 times (for example, 36 hyperparameter combinations * 5 = 135 fits in total) and check the results.\n",
    "\n",
    "> **Note:** Depending on the compute power of the machine you're using, the following cell may take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlI5FKQ0pTeO",
    "outputId": "4002c91a-d7f9-4996-bfe8-b343fd884c02",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start the timer\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into X & y\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Set n_jobs to -1 to use all available machine cores (if this produces errors, try n_jobs=1)\n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs_clf = GridSearchCV(estimator=clf,\n",
    "                      param_grid=param_grid,\n",
    "                      cv=5, # 5-fold cross-validation\n",
    "                      verbose=2) # print out progress\n",
    "\n",
    "# Fit the RandomizedSearchCV version of clf\n",
    "gs_clf.fit(X_train, y_train);\n",
    "\n",
    "# Find the running time\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9OJ_Y3HpTeO",
    "outputId": "4acb25cc-cb7c-457c-f899-7134851dfa48"
   },
   "outputs": [],
   "source": [
    "# How long did it take?\n",
    "total_time = end_time - start_time\n",
    "print(f\"[INFO] The total running time for running GridSearchCV was {total_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8M2E-17IpTeO"
   },
   "source": [
    "Once it completes, we can check the best hyperparameter combinations it found using the `best_params_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCnlXq5RpTeO",
    "outputId": "83c2a41d-f5d1-43ea-a90e-509c981cdb19"
   },
   "outputs": [],
   "source": [
    "# Check the best hyperparameters found with GridSearchCV\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DR5KEYC8pTeO"
   },
   "source": [
    "And by default when we call the `predict()` function on `gs_clf`, it'll use the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdGkqubRpTeP",
    "outputId": "6a5dcc85-2c37-44c1-afd9-115583e68015"
   },
   "outputs": [],
   "source": [
    "# Max predictions with the GridSearchCV classifier\n",
    "gs_y_preds = gs_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "gs_metrics = evaluate_preds(y_test, gs_y_preds)\n",
    "gs_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbbX4xnIpTeP"
   },
   "source": [
    "Let's create a DataFrame to compare the different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0P8fw0eZpTeP",
    "outputId": "088e0474-1a77-4fa3-f5e5-9d07a71a9541"
   },
   "outputs": [],
   "source": [
    "compare_metrics = pd.DataFrame({\"baseline\": baseline_metrics,\n",
    "                                \"clf_2\": clf_2_metrics,\n",
    "                                \"random search\": rs_metrics,\n",
    "                                \"grid search\": gs_metrics})\n",
    "compare_metrics.plot.bar(figsize=(10, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyssCscEpTeP"
   },
   "source": [
    "Nice!\n",
    "\n",
    "After trying many different combinations of hyperparamters, we get a slight improvement in results.\n",
    "\n",
    "However, sometimes you'll notice that your results don't change much.\n",
    "\n",
    "These things might happen.\n",
    "\n",
    "But it's important to remember, it's not over. There more things you can try.\n",
    "\n",
    "In a hyperparameter tuning sense, there may be a better set we could find through more extensive searching with `RandomizedSearchCV` and `GridSearchCV`, this would require more experimentation.\n",
    "\n",
    "Other techniques you could:\n",
    "* **Collecting more data** - Based on the results our models are getting now, it seems like they're very capable of finding patterns. Collecting more data may improve a models ability to find patterns. However, your ability to do this will largely depend on the project you're working on.\n",
    "* **Try a more advanced model** - Although our tuned Random Forest model is doing pretty well, a more advanced ensemble method such as [XGBoost](https://xgboost.ai/) or [CatBoost](https://catboost.ai/) might perform better. I'll leave these for extra-curriculum.\n",
    "\n",
    "Since machine learning is part engineering, part science, these kind of experiments are common place in any machine learning project.\n",
    "\n",
    "Now we've got a tuned Random Forest model, let's find out how we might save it and export it so we can share it with others or potentially use it in an external application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuGzlMQFpTeP"
   },
   "source": [
    "## 6. Saving and loading trained machine learning models\n",
    "\n",
    "Our `GridSearchCV` model (`gs_clf`) has the best results so far, we'll export it and save it to file.\n",
    "\n",
    "### 6.1 Saving and loading a model with `pickle`\n",
    "\n",
    "We saw right at the start, one way to save a model is using Python's [`pickle` module](https://docs.python.org/3/library/pickle.html).\n",
    "\n",
    "We'll use `pickle`'s `dump()` method and pass it our model, `gs_clf`, along with the `open()` function containing a string for the filename we want to save our model as, along with the `\"wb\"` string which stands for \"write binary\", which is the file type `open()` will write our model as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCflQ7GgpTeP"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save an existing model to file\n",
    "best_model_file_name_pickle = \"gs_random_forest_model_1.pkl\" # .pkl extension stands for \"pickle\"\n",
    "pickle.dump(gs_clf, open(best_model_file_name_pickle, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbvjjHQzpTeQ"
   },
   "source": [
    "Once it's saved, we can import it using `pickle`'s `load()` function, passing it `open()` containing the filename as a string and `\"rb\"` standing for \"read binary\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbm_veEWpTeQ"
   },
   "outputs": [],
   "source": [
    "# Load a saved model\n",
    "loaded_pickle_model = pickle.load(open(best_model_file_name_pickle, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgOL2zzXpTeQ"
   },
   "source": [
    "Once you've reimported your trained model using `pickle`, you can use it to make predictions as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBf3l1wtpTeQ",
    "outputId": "3ff38dd9-1073-490b-c5cd-5273743d5746"
   },
   "outputs": [],
   "source": [
    "# Make predictions and evaluate the loaded model\n",
    "pickle_y_preds = loaded_pickle_model.predict(X_test)\n",
    "loaded_pickle_model_metrics = evaluate_preds(y_test, pickle_y_preds)\n",
    "loaded_pickle_model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WhcSuhPpTeQ"
   },
   "source": [
    "You'll notice the reimported model evaluation metrics are the same as the model before we exported it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSFPzHLfpTeQ",
    "outputId": "43681009-6e0f-4d9f-e919-eeb3a81d86c7"
   },
   "outputs": [],
   "source": [
    "loaded_pickle_model_metrics == gs_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkUUbRMQpTeR"
   },
   "source": [
    "### 6.2 Saving and loading a model with [`joblib`](https://joblib.readthedocs.io/en/latest/persistence.html)\n",
    "\n",
    "The other way to load and save models is with `joblib`. Which works relatively the same as `pickle`.\n",
    "\n",
    "To save a model, we can use `joblib`'s `dump()` function, passing it the model (`gs_clf`) and the desired filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQncx6x7pTeR",
    "outputId": "77cc8188-4170-456b-a1d1-23523389bb6b"
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Save a model to file\n",
    "best_model_file_name_joblib = \"gs_random_forest_model_1.joblib\"\n",
    "dump(gs_clf, filename=best_model_file_name_joblib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssgjBa1NpTeR"
   },
   "source": [
    "Once you've saved a model using `dump()`, you can import it using `load()` and passing it the filename of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6rO8iVppTeR"
   },
   "outputs": [],
   "source": [
    "# Import a saved joblib model\n",
    "loaded_joblib_model = load(filename=best_model_file_name_joblib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9J7dJZ0pTeR"
   },
   "source": [
    "Again, once imported, we can make predictions with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3y0JCyLpTeR",
    "outputId": "3508b9f2-dd0b-4a90-fe96-5035336537c2"
   },
   "outputs": [],
   "source": [
    "# Make and evaluate joblib predictions\n",
    "joblib_y_preds = loaded_joblib_model.predict(X_test)\n",
    "loaded_joblib_model_metrics = evaluate_preds(y_test, joblib_y_preds)\n",
    "loaded_joblib_model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6-pv3DypTeR"
   },
   "source": [
    "And once again, you'll notice the evaluation metrics are the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2wzevR1pTeS",
    "outputId": "06a67119-00c3-45b4-b659-58c375bb1e21"
   },
   "outputs": [],
   "source": [
    "loaded_joblib_model_metrics == gs_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2W2JuK87pTeS"
   },
   "source": [
    "So which one should you use, `pickle` or `joblib`?\n",
    "\n",
    "According to [Scikit-Learn's model persistence documentation](https://scikit-learn.org/stable/model_persistence.html), they suggest it may be more efficient to use `joblib` as it's more efficient with large numpy arrays (which is what may be contained in trained/fitted Scikit-Learn models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTCaOjX5pTeS"
   },
   "source": [
    "## 7. Revisiting the Entire Pipeline\r\n",
    "\r\n",
    "I've covered a lot so far, and it might seem a bit all over the place. But that's okay—machine learning projects often start out this way.\r\n",
    "\r\n",
    "At the beginning, there's a lot of experimenting and code scattered around. Once something starts to work, the refinement process begins.\r\n",
    "\r\n",
    "What does this refinement process look like?\r\n",
    "\r\n",
    "I'll use the car sales regression problem (predicting the sale price of cars) as an example.\r\n",
    "\r\n",
    "To tidy things up, I'll be using Scikit-Learn's [`sklearn.pipeline.Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class.\r\n",
    "\r\n",
    "I can think of `Pipeline` as a way to string together a number of different Scikit-Learn processes ina sequence.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_2NU7CgpTeS"
   },
   "source": [
    "### 7.1 Creating a regression [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "You might recall when, way back in Section 2: Getting Data Ready, we dealt with the car sales data, to build a regression model on it, we had to encode the categorical features into numbers and fill the missing data.\n",
    "\n",
    "The code we used worked, but it was a bit all over the place.\n",
    "\n",
    "Good news is, `Pipeline` can help us clean it up.\n",
    "\n",
    "Let's remind ourselves what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVadHHwjpTeS",
    "outputId": "df2d9529-f715-4af8-b795-86fb34065f59"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/car-sales-extended-missing-data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2dQBfznpTeS",
    "outputId": "183a0a2c-44b3-4a68-ca40-453272f354d1"
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpaijhUapTeS",
    "outputId": "4578efc7-5163-49d1-e818-f1c9cd08765e"
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJEBte_2pTeS"
   },
   "source": [
    "There's 1000 rows, three features are categorical (`Make`, `Colour`, `Doors`), the other two are numerical (`Odometer (KM)`, `Price`) and there's 249 missing values.\n",
    "\n",
    "We're going to have to turn the categorical features into numbers and fill the missing values before we can fit a model.\n",
    "\n",
    "We'll build a `Pipeline` to do so.\n",
    "\n",
    "`Pipeline`'s main input parameter is `steps` which is a list of tuples (`[(step_name, action_to_take)]`) of the step name, plus the action you'd like it to perform.\n",
    "\n",
    "In our case, you could think of the steps as:\n",
    "1. Fill missing data\n",
    "2. Convert data to numbers\n",
    "3. Build a model on the data\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTRkEMzSpTeT",
    "outputId": "37b03a54-0807-4421-9ef8-3edecfb25ebe"
   },
   "outputs": [],
   "source": [
    "# Getting data ready\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Setup random seed\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import data and drop the rows with missing labels\n",
    "data = pd.read_csv(\"data/car-sales-extended-missing-data.csv\")\n",
    "data.dropna(subset=[\"Price\"], inplace=True)\n",
    "\n",
    "# Define different features and transformer pipelines\n",
    "categorical_features = [\"Make\", \"Colour\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "door_feature = [\"Doors\"]\n",
    "door_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=4))])\n",
    "\n",
    "numeric_features = [\"Odometer (KM)\"]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))\n",
    "])\n",
    "\n",
    "# Setup preprocessing steps (fill missing values, then convert to numbers)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "        (\"door\", door_transformer, door_feature),\n",
    "        (\"num\", numeric_transformer, numeric_features)])\n",
    "\n",
    "# Create a preprocessing and modelling pipeline\n",
    "model = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
    "                        (\"model\", RandomForestRegressor(n_jobs=-1))])\n",
    "\n",
    "# Split data\n",
    "X = data.drop(\"Price\", axis=1)\n",
    "y = data[\"Price\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Fit and score the model\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWBk_nlqpTeT"
   },
   "source": [
    "What we've done is combine a series of data preprocessing steps (filling missing values, encoding numerical values) as well as a model into a `Pipeline`.\n",
    "\n",
    "Doing so not only cleans up the code, it ensures the same steps are taken every time the code is run rather than having multiple different processing steps happening in different stages.\n",
    "\n",
    "It's also possible to `GridSearchCV` or `RandomizedSearchCV` with a `Pipeline`.\n",
    "\n",
    "The main difference is when creating a hyperparameter grid, you have to add a prefix to each hyperparameter (see the [documentation for `RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) for a full list of possible hyperparameters to tune).\n",
    "\n",
    "The prefix is the name of the `Pipeline` step you'd like to alter, followed by two underscores.\n",
    "\n",
    "For example, to adjust `n_estimators` of `\"model\"` in the `Pipeline`, you'd use: `\"model__n_estimators\"` (note the double underscore after `model__` at the start).\n",
    "\n",
    "Let's see it!\n",
    "\n",
    "> **Note:** Depending on your computer's processing power, the cell below may take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sp8BcYrpTeT",
    "outputId": "e2190600-9dd2-4eff-fa99-a964cc07bc40"
   },
   "outputs": [],
   "source": [
    "# Using grid search with pipeline\n",
    "pipe_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"], # note the double underscore after each prefix \"preprocessor__\"\n",
    "    \"model__n_estimators\": [100, 1000],\n",
    "    \"model__max_depth\": [None, 5],\n",
    "    \"model__max_features\": [\"sqrt\"],\n",
    "    \"model__min_samples_split\": [2, 4]\n",
    "}\n",
    "\n",
    "gs_model = GridSearchCV(model, pipe_grid, cv=5, verbose=2)\n",
    "gs_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FclVaULdpTeT"
   },
   "source": [
    "Now let's find the score of our model (by default `GridSearchCV` saves the best model to the `gs_model` object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GFNaYenpTeT",
    "outputId": "570e6d1f-608a-48df-b6db-60dda0cc1e25"
   },
   "outputs": [],
   "source": [
    "# Score the best model\n",
    "gs_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVvQIBqcpTeT"
   },
   "source": [
    "Beautiful!\n",
    "\n",
    "Using `GridSearchCV` we see a nice boost in our models score.\n",
    "\n",
    "And the best thing is, because it's all in a `Pipeline`, we could easily replicate these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on making it this far! Throughout this notebook, we've covered a wide range of topics within the Scikit-Learn library, including data preparation, model selection, fitting models, evaluating performance, and even experimenting with different approaches.\n",
    "\n",
    "As you continue your journey in machine learning, there are several resources you might find useful:\n",
    "\n",
    "- **Kaggle Competitions**: Applying what you've learned to a [Kaggle competition](https://www.kaggle.com) can be a great way to practice your skills. For instance, you could try combining the heart disease classification code and the `Pipeline` code to build a model for the [Titanic dataset](https://www.kaggle.com/c/titanic).\n",
    "\n",
    "- **Scikit-Learn User Guide**: For anyone interested in exploring more about what the Scikit-Learn library can do, the [Scikit-Learn User Guide](https://scikit-learn.org/stable/user_guide.html) is a fantastic resource. It’s a great place to discover new features and techniques that might spark your interest.\n",
    "\n",
    "- **CatBoost Library**: If you're looking to handle non-numerical data automatically, you might want to explore the [CatBoost library](https://catboost.ai). CatBoost is an advanced version of a decision tree algorithm and is used in production at several large technology companies. You can read more about its use in real-world applications [here](https://blog.cloudflare.com/how-cloudflare-runs-ml-inference-in-microseconds/).\n",
    "\n",
    "- **Machine Learning Books**: Books such as \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" by Aurélien Géron can provide in-depth knowledge and practical examples.\n",
    "\n",
    "- **Online Courses**: Platforms like Coursera, edX, and Udacity offer comprehensive courses on machine learning and data science that can further enhance your understanding and skills.\n",
    "\n",
    "The journey of learning machine learning is ongoing, and every step you take builds a stronger foundation for future exploration and discovery. Keep experimenting, keep learning, and don't hesitate to dive into new resources to expand your knowledge.\n",
    "\n",
    "Happy learning!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzv3gSo8uO_c"
   },
   "source": [
    "## Acknowledgment\n",
    "\n",
    "This notebook was created based on the course [Complete A.I. & Machine Learning, Data Science Bootcamp](https://www.udemy.com/share/102vAM3@K78pqhxD1f2zzD96QleliyDouLGw5kKgFgFJcvA7iOUPhe3-F7BEDtaAS8qgF7yUQQ==/) by Andrei Neagoie and Daniel Bourke. The course provided a comprehensive introduction to machine learning concepts and practical implementations using Python and Scikit-Learn.\n",
    "\n",
    "Thank you to the instructor for the valuable lessons and guidance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c556f6eb84b27a92005489cdcf9c9b80cc62ee891441f20eabfc5ad7282165a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
